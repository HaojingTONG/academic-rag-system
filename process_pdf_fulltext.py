#!/usr/bin/env python3
"""
PDFå…¨æ–‡å¤„ç†è„šæœ¬
æ‰¹é‡å¤„ç†raw_papersä¸­çš„PDFæ–‡ä»¶ï¼Œæå–å®Œæ•´å†…å®¹å¹¶æ›´æ–°è®ºæ–‡æ•°æ®
"""

import json
import sys
from pathlib import Path
from typing import Dict, List
import time

# æ·»åŠ srcåˆ°è·¯å¾„  
sys.path.append(str(Path(__file__).parent))
sys.path.append(str(Path(__file__).parent / 'src'))

from src.processor.pdf_processor import AcademicPDFProcessor
from src.processor.document_chunker import DocumentChunker, ChunkingConfig

def process_pdf_fulltext():
    """å¤„ç†æ‰€æœ‰PDFçš„å…¨æ–‡å†…å®¹"""
    print("ğŸš€ å¼€å§‹PDFå…¨æ–‡å¤„ç†")
    print("=" * 80)
    
    # åˆå§‹åŒ–å¤„ç†å™¨
    pdf_processor = AcademicPDFProcessor()
    
    # é…ç½®æ™ºèƒ½åˆ†å—å™¨
    chunking_config = ChunkingConfig(
        strategy='hybrid',
        chunk_size=600,
        chunk_overlap=100,
        min_chunk_size=100,
        preserve_sentences=True,
        section_aware=True
    )
    chunker = DocumentChunker(chunking_config)
    
    # åŠ è½½ç°æœ‰è®ºæ–‡æ•°æ®
    papers_file = Path("data/main_system_papers.json")
    if not papers_file.exists():
        print("âŒ è®ºæ–‡æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: data/main_system_papers.json")
        return False
    
    with open(papers_file, 'r', encoding='utf-8') as f:
        papers_data = json.load(f)
    
    print(f"ğŸ“š åŠ è½½è®ºæ–‡æ•°æ®: {len(papers_data)} ç¯‡")
    
    # PDFæ–‡ä»¶ç›®å½•
    pdf_dir = Path("data/raw_papers")
    if not pdf_dir.exists():
        print("âŒ PDFç›®å½•ä¸å­˜åœ¨: data/raw_papers")
        return False
    
    pdf_files = {f.stem: f for f in pdf_dir.glob("*.pdf")}
    print(f"ğŸ“„ æ‰¾åˆ°PDFæ–‡ä»¶: {len(pdf_files)} ä¸ª")
    
    # ç»Ÿè®¡ä¿¡æ¯
    processed_count = 0
    enhanced_count = 0
    failed_count = 0
    
    # å¤„ç†æ¯ç¯‡è®ºæ–‡
    for i, paper in enumerate(papers_data):
        paper_id = paper.get('id', '')
        title = paper.get('title', 'Unknown')[:50]
        
        print(f"\nğŸ“– [{i+1}/{len(papers_data)}] {title}...")
        print(f"   ID: {paper_id}")
        
        # æ£€æŸ¥æ˜¯å¦æœ‰å¯¹åº”çš„PDFæ–‡ä»¶
        if paper_id not in pdf_files:
            print(f"   âš ï¸ æœªæ‰¾åˆ°PDFæ–‡ä»¶: {paper_id}.pdf")
            continue
        
        pdf_path = pdf_files[paper_id]
        
        # æ£€æŸ¥æ˜¯å¦å·²ç»å¤„ç†è¿‡å…¨æ–‡
        existing_chunks = paper.get('processed_chunks', [])
        if len(existing_chunks) > 1:
            section_types = [chunk['metadata']['section_type'] for chunk in existing_chunks]
            if any(stype != 'abstract' for stype in section_types):
                print(f"   âœ… å·²å¤„ç†å…¨æ–‡ ({len(existing_chunks)} å—)")
                processed_count += 1
                continue
        
        # æå–PDFå…¨æ–‡å†…å®¹
        print(f"   ğŸ”„ æå–PDFå…¨æ–‡...")
        pdf_content = pdf_processor.extract_pdf_content(str(pdf_path))
        
        if not pdf_content:
            print(f"   âŒ PDFæå–å¤±è´¥")
            failed_count += 1
            continue
        
        # å‡†å¤‡æ–‡æ¡£ç”¨äºåˆ†å—
        documents_for_chunking = []
        
        # æ·»åŠ æ‘˜è¦ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
        if pdf_content.abstract:
            documents_for_chunking.append({
                'content': f"Title: {pdf_content.title}\n\nAbstract: {pdf_content.abstract}",
                'metadata': {
                    'section_type': 'abstract',
                    'title': pdf_content.title
                }
            })
        
        # æ·»åŠ å„ä¸ªç« èŠ‚
        for section in pdf_content.sections:
            if section.content.strip() and len(section.content) > 200:  # è¿‡æ»¤å¤ªçŸ­çš„ç« èŠ‚
                documents_for_chunking.append({
                    'content': f"Section: {section.title}\n\n{section.content}",
                    'metadata': {
                        'section_type': section.section_type,
                        'section_title': section.title,
                        'page_range': f"{section.page_range[0]}-{section.page_range[1]}",
                        'confidence': section.confidence
                    }
                })
        
        if not documents_for_chunking:
            print(f"   âŒ æ— æœ‰æ•ˆå†…å®¹å¯å¤„ç†")
            failed_count += 1
            continue
        
        print(f"   ğŸ“‘ å‡†å¤‡åˆ†å—: {len(documents_for_chunking)} ä¸ªç« èŠ‚")
        
        # æ™ºèƒ½åˆ†å—å¤„ç†
        all_chunks = []
        for doc in documents_for_chunking:
            try:
                chunks = chunker.chunk_document(
                    text=doc['content'],
                    paper_id=paper_id,
                    metadata=doc['metadata']
                )
                all_chunks.extend(chunks)
            except Exception as e:
                print(f"   âš ï¸ åˆ†å—å¤±è´¥: {e}")
                continue
        
        if not all_chunks:
            print(f"   âŒ åˆ†å—å¤„ç†å¤±è´¥")
            failed_count += 1
            continue
        
        # è½¬æ¢ä¸ºè®ºæ–‡æ•°æ®æ ¼å¼
        processed_chunks = []
        for chunk in all_chunks:
            chunk_data = {
                'text': chunk.text,
                'metadata': {
                    'chunk_id': chunk.chunk_id,
                    'paper_id': paper_id,
                    'title': pdf_content.title,
                    'section_type': chunk.metadata.get('section_type', 'content'),
                    'word_count': chunk.word_count,
                    'char_count': chunk.char_count,
                    'has_formulas': pdf_content.has_formulas,
                    'has_code': False,  # å¯ä»¥åç»­æ‰©å±•
                    'has_citations': 'references' in chunk.text.lower(),
                    'section_title': chunk.metadata.get('section_title', ''),
                    'page_range': chunk.metadata.get('page_range', ''),
                    'language': pdf_content.language,
                    'total_pages': pdf_content.total_pages,
                    'processing_version': '2.0'  # æ ‡è®°ä¸ºå…¨æ–‡å¤„ç†ç‰ˆæœ¬
                }
            }
            processed_chunks.append(chunk_data)
        
        # æ›´æ–°è®ºæ–‡æ•°æ®
        paper['processed_chunks'] = processed_chunks
        paper['pdf_processed'] = True
        paper['pdf_stats'] = {
            'total_pages': pdf_content.total_pages,
            'total_words': pdf_content.total_words,
            'sections_count': len(pdf_content.sections),
            'chunks_count': len(processed_chunks),
            'has_formulas': pdf_content.has_formulas,
            'has_tables': pdf_content.has_tables,
            'has_figures': pdf_content.has_figures,
            'language': pdf_content.language
        }
        
        enhanced_count += 1
        print(f"   âœ… å…¨æ–‡å¤„ç†å®Œæˆ: {len(processed_chunks)} ä¸ªæ–‡æ¡£å—")
        
        # æ¯å¤„ç†10ç¯‡è®ºæ–‡ä¿å­˜ä¸€æ¬¡
        if enhanced_count % 10 == 0:
            print(f"\nğŸ’¾ ä¸­é—´ä¿å­˜... (å·²å¤„ç† {enhanced_count} ç¯‡)")
            with open(papers_file, 'w', encoding='utf-8') as f:
                json.dump(papers_data, f, ensure_ascii=False, indent=2)
    
    # æœ€ç»ˆä¿å­˜
    print(f"\nğŸ’¾ ä¿å­˜æœ€ç»ˆç»“æœ...")
    with open(papers_file, 'w', encoding='utf-8') as f:
        json.dump(papers_data, f, ensure_ascii=False, indent=2)
    
    # ç”Ÿæˆç»Ÿè®¡æŠ¥å‘Š
    print(f"\n" + "=" * 80)
    print(f"ğŸ“Š PDFå…¨æ–‡å¤„ç†å®Œæˆ!")
    print(f"=" * 80)
    print(f"âœ… æ–°å¤„ç†è®ºæ–‡: {enhanced_count} ç¯‡")
    print(f"âœ… å·²å¤„ç†è®ºæ–‡: {processed_count} ç¯‡") 
    print(f"âŒ å¤„ç†å¤±è´¥: {failed_count} ç¯‡")
    print(f"ğŸ“š æ€»è®ºæ–‡æ•°: {len(papers_data)} ç¯‡")
    
    if enhanced_count > 0:
        print(f"\nğŸ¯ å¤„ç†æ•ˆæœ:")
        
        # ç»Ÿè®¡å¹³å‡æ–‡æ¡£å—æ•°
        total_chunks = 0
        enhanced_papers = 0
        
        for paper in papers_data:
            chunks = paper.get('processed_chunks', [])
            if len(chunks) > 1 and paper.get('pdf_processed', False):
                total_chunks += len(chunks)
                enhanced_papers += 1
        
        if enhanced_papers > 0:
            avg_chunks = total_chunks / enhanced_papers
            print(f"   ğŸ“‘ å¹³å‡æ–‡æ¡£å—æ•°: {avg_chunks:.1f} ä¸ª/è®ºæ–‡")
            print(f"   ğŸš€ æ£€ç´¢è´¨é‡å°†æ˜¾è‘—æå‡!")
    
    return enhanced_count > 0

def main():
    """ä¸»å‡½æ•°"""
    try:
        success = process_pdf_fulltext()
        return 0 if success else 1
    except KeyboardInterrupt:
        print(f"\n\nâ¹ï¸ ç”¨æˆ·ä¸­æ–­å¤„ç†")
        return 1
    except Exception as e:
        print(f"\nâŒ å¤„ç†è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
        return 1

if __name__ == "__main__":
    sys.exit(main())