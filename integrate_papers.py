#!/usr/bin/env python3
"""
è®ºæ–‡æ•°æ®æ•´åˆå™¨
å°†æ–°æ”¶é›†çš„é«˜è´¨é‡è®ºæ–‡ä¸ç°æœ‰è®ºæ–‡æ•°æ®åˆå¹¶ï¼Œå»é‡å¹¶ä¼˜åŒ–
"""

import json
import os
from typing import List, Dict, Set
from datetime import datetime

class PaperIntegrator:
    """è®ºæ–‡æ•°æ®æ•´åˆå™¨"""
    
    def __init__(self):
        self.existing_papers = []
        self.new_papers = []
        self.integrated_papers = []
        self.duplicate_count = 0
        
    def load_existing_papers(self, file_path: str = "data/papers_info.json") -> bool:
        """åŠ è½½ç°æœ‰è®ºæ–‡æ•°æ®"""
        if os.path.exists(file_path):
            try:
                with open(file_path, 'r', encoding='utf-8') as f:
                    self.existing_papers = json.load(f)
                print(f"âœ… åŠ è½½ç°æœ‰è®ºæ–‡: {len(self.existing_papers)} ç¯‡")
                return True
            except Exception as e:
                print(f"âŒ åŠ è½½ç°æœ‰è®ºæ–‡å¤±è´¥: {e}")
                return False
        else:
            print("ğŸ“ æœªæ‰¾åˆ°ç°æœ‰è®ºæ–‡æ•°æ®ï¼Œå°†åˆ›å»ºæ–°æ–‡ä»¶")
            return True
    
    def load_new_papers(self, file_path: str = "data/high_quality_papers.json") -> bool:
        """åŠ è½½æ–°æ”¶é›†çš„è®ºæ–‡æ•°æ®"""
        if not os.path.exists(file_path):
            print(f"âŒ æœªæ‰¾åˆ°æ–°è®ºæ–‡æ•°æ®æ–‡ä»¶: {file_path}")
            print("è¯·å…ˆè¿è¡Œ: python collect_high_quality_papers.py")
            return False
        
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
                
            # æ”¯æŒä¸¤ç§æ ¼å¼
            if 'papers' in data:
                self.new_papers = data['papers']
                stats = data.get('statistics', {})
                print(f"âœ… åŠ è½½æ–°æ”¶é›†è®ºæ–‡: {len(self.new_papers)} ç¯‡")
                print(f"   å¹³å‡å¼•ç”¨é‡: {stats.get('avg_citations', 0):.1f}")
                print(f"   å¹³å‡è´¨é‡åˆ†æ•°: {stats.get('avg_quality_score', 0):.2f}")
            else:
                self.new_papers = data
                print(f"âœ… åŠ è½½æ–°æ”¶é›†è®ºæ–‡: {len(self.new_papers)} ç¯‡")
            
            return True
            
        except Exception as e:
            print(f"âŒ åŠ è½½æ–°è®ºæ–‡å¤±è´¥: {e}")
            return False
    
    def _normalize_title(self, title: str) -> str:
        """æ ‡å‡†åŒ–æ ‡é¢˜ç”¨äºå»é‡"""
        if not title:
            return ""
        
        # è½¬å°å†™ï¼Œç§»é™¤æ ‡ç‚¹ç¬¦å·å’Œå¤šä½™ç©ºæ ¼
        import re
        normalized = re.sub(r'[^\w\s]', ' ', title.lower())
        normalized = ' '.join(normalized.split())
        return normalized
    
    def _are_papers_duplicate(self, paper1: Dict, paper2: Dict) -> bool:
        """æ£€æŸ¥ä¸¤ç¯‡è®ºæ–‡æ˜¯å¦é‡å¤"""
        
        # 1. æ£€æŸ¥ID
        id1 = paper1.get('id', '')
        id2 = paper2.get('id', '')
        if id1 and id2 and id1 == id2:
            return True
        
        # 2. æ£€æŸ¥æ ‡é¢˜ç›¸ä¼¼åº¦
        title1 = self._normalize_title(paper1.get('title', ''))
        title2 = self._normalize_title(paper2.get('title', ''))
        
        if not title1 or not title2:
            return False
        
        # ç®€å•çš„å­—ç¬¦ä¸²ç›¸ä¼¼åº¦æ£€æŸ¥
        if title1 == title2:
            return True
        
        # æ£€æŸ¥æ ‡é¢˜åŒ…å«å…³ç³»ï¼ˆå¤„ç†æ ‡é¢˜é•¿çŸ­ä¸ä¸€çš„æƒ…å†µï¼‰
        if len(title1) > len(title2):
            longer, shorter = title1, title2
        else:
            longer, shorter = title2, title1
        
        # å¦‚æœçŸ­æ ‡é¢˜æ˜¯é•¿æ ‡é¢˜çš„å­é›†ä¸”ç›¸ä¼¼åº¦å¾ˆé«˜
        if shorter in longer and len(shorter) / len(longer) > 0.8:
            return True
        
        return False
    
    def integrate_papers(self) -> List[Dict]:
        """æ•´åˆè®ºæ–‡æ•°æ®"""
        print("\nå¼€å§‹æ•´åˆè®ºæ–‡æ•°æ®...")
        
        # åˆ›å»ºç°æœ‰è®ºæ–‡çš„æ ‡é¢˜ç´¢å¼•
        existing_titles = set()
        for paper in self.existing_papers:
            title = self._normalize_title(paper.get('title', ''))
            if title:
                existing_titles.add(title)
        
        self.integrated_papers = self.existing_papers.copy()
        
        # å¤„ç†æ–°è®ºæ–‡
        for new_paper in self.new_papers:
            is_duplicate = False
            
            # æ£€æŸ¥æ˜¯å¦ä¸ç°æœ‰è®ºæ–‡é‡å¤
            for existing_paper in self.existing_papers:
                if self._are_papers_duplicate(new_paper, existing_paper):
                    is_duplicate = True
                    self.duplicate_count += 1
                    break
            
            if not is_duplicate:
                # æ ‡å‡†åŒ–æ–°è®ºæ–‡æ ¼å¼
                standardized_paper = self._standardize_paper_format(new_paper)
                if standardized_paper:
                    self.integrated_papers.append(standardized_paper)
        
        print(f"âœ… æ•´åˆå®Œæˆ:")
        print(f"   åŸæœ‰è®ºæ–‡: {len(self.existing_papers)} ç¯‡")
        print(f"   æ–°å¢è®ºæ–‡: {len(self.integrated_papers) - len(self.existing_papers)} ç¯‡")
        print(f"   é‡å¤è®ºæ–‡: {self.duplicate_count} ç¯‡")
        print(f"   æ€»è®¡è®ºæ–‡: {len(self.integrated_papers)} ç¯‡")
        
        return self.integrated_papers
    
    def _standardize_paper_format(self, paper: Dict) -> Dict:
        """æ ‡å‡†åŒ–è®ºæ–‡æ ¼å¼"""
        try:
            # ç¡®ä¿å¿…éœ€å­—æ®µå­˜åœ¨
            standardized = {
                'id': paper.get('id', f"paper_{hash(paper.get('title', ''))}"[:16]),
                'title': paper.get('title', '').strip(),
                'abstract': paper.get('abstract', '').strip(),
                'authors': paper.get('authors', []),
                'published': paper.get('published', paper.get('publicationDate', '')),
                'venue': paper.get('venue', ''),
                'url': paper.get('url', ''),
                'source': paper.get('source', 'collected')
            }
            
            # æ·»åŠ è´¨é‡æŒ‡æ ‡ï¼ˆå¦‚æœæœ‰çš„è¯ï¼‰
            if 'citationCount' in paper:
                standardized['citationCount'] = paper['citationCount']
            if 'quality_score' in paper:
                standardized['quality_score'] = paper['quality_score']
            if 'year' in paper:
                standardized['year'] = paper['year']
            
            # éªŒè¯å¿…è¦å­—æ®µ
            if not standardized['title'] or len(standardized['title']) < 5:
                return None
            
            if not standardized['abstract'] or len(standardized['abstract']) < 20:
                return None
            
            return standardized
            
        except Exception as e:
            print(f"æ ‡å‡†åŒ–è®ºæ–‡æ ¼å¼å¤±è´¥: {e}")
            return None
    
    def save_integrated_papers(self, output_file: str = "data/papers_info.json"):
        """ä¿å­˜æ•´åˆåçš„è®ºæ–‡æ•°æ®"""
        
        # æŒ‰è´¨é‡æ’åºï¼ˆå¦‚æœæœ‰è´¨é‡æŒ‡æ ‡çš„è¯ï¼‰
        def sort_key(paper):
            citation_count = paper.get('citationCount', 0)
            quality_score = paper.get('quality_score', 0)
            year = paper.get('year', 2020)
            
            # ç»¼åˆæ’åºï¼šå¼•ç”¨é‡ * 0.6 + è´¨é‡åˆ†æ•° * 0.3 + å¹´ä»½æƒé‡ * 0.1
            year_weight = (year - 2015) / 10 if year else 0  # å¹´ä»½æƒé‡
            total_score = citation_count * 0.6 + quality_score * 100 * 0.3 + year_weight * 10
            return total_score
        
        self.integrated_papers.sort(key=sort_key, reverse=True)
        
        # åˆ›å»ºå¤‡ä»½
        if os.path.exists(output_file):
            backup_file = f"{output_file}.backup.{datetime.now().strftime('%Y%m%d_%H%M%S')}"
            os.rename(output_file, backup_file)
            print(f"ğŸ“‹ åˆ›å»ºå¤‡ä»½: {backup_file}")
        
        # ä¿å­˜æ•´åˆæ•°æ®
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(self.integrated_papers, f, ensure_ascii=False, indent=2)
        
        print(f"ğŸ’¾ æ•´åˆæ•°æ®å·²ä¿å­˜: {output_file}")
        
        # ç”Ÿæˆç»Ÿè®¡æŠ¥å‘Š
        self._generate_statistics_report()
        
        return output_file
    
    def _generate_statistics_report(self):
        """ç”Ÿæˆç»Ÿè®¡æŠ¥å‘Š"""
        if not self.integrated_papers:
            return
        
        print(f"\nğŸ“Š æ•´åˆæ•°æ®ç»Ÿè®¡æŠ¥å‘Š:")
        print("=" * 50)
        
        # åŸºç¡€ç»Ÿè®¡
        total_papers = len(self.integrated_papers)
        papers_with_citations = sum(1 for p in self.integrated_papers if p.get('citationCount', 0) > 0)
        
        print(f"è®ºæ–‡æ€»æ•°: {total_papers}")
        print(f"æœ‰å¼•ç”¨æ•°æ®: {papers_with_citations} ç¯‡")
        
        # å¼•ç”¨é‡ç»Ÿè®¡
        if papers_with_citations > 0:
            citations = [p.get('citationCount', 0) for p in self.integrated_papers if p.get('citationCount', 0) > 0]
            avg_citations = sum(citations) / len(citations)
            max_citations = max(citations)
            print(f"å¹³å‡å¼•ç”¨é‡: {avg_citations:.1f}")
            print(f"æœ€é«˜å¼•ç”¨é‡: {max_citations}")
            
            # é«˜å¼•ç”¨è®ºæ–‡ç»Ÿè®¡
            high_citation_papers = [p for p in self.integrated_papers if p.get('citationCount', 0) >= 100]
            print(f"é«˜å¼•ç”¨è®ºæ–‡ (â‰¥100): {len(high_citation_papers)} ç¯‡")
        
        # å¹´ä»½åˆ†å¸ƒ
        year_dist = {}
        for paper in self.integrated_papers:
            year = paper.get('year')
            if year:
                year_dist[year] = year_dist.get(year, 0) + 1
        
        if year_dist:
            print(f"\nå¹´ä»½åˆ†å¸ƒ:")
            for year in sorted(year_dist.keys(), reverse=True)[:5]:
                print(f"  {year}: {year_dist[year]} ç¯‡")
        
        # æ¥æºç»Ÿè®¡
        source_dist = {}
        for paper in self.integrated_papers:
            source = paper.get('source', 'unknown')
            source_dist[source] = source_dist.get(source, 0) + 1
        
        if source_dist:
            print(f"\næ•°æ®æ¥æº:")
            for source, count in sorted(source_dist.items(), key=lambda x: x[1], reverse=True):
                print(f"  {source}: {count} ç¯‡")
        
        # è´¨é‡åˆ†å¸ƒ
        quality_papers = [p for p in self.integrated_papers if 'quality_score' in p]
        if quality_papers:
            avg_quality = sum(p['quality_score'] for p in quality_papers) / len(quality_papers)
            print(f"\nè´¨é‡è¯„åˆ†:")
            print(f"  å¹³å‡è´¨é‡åˆ†æ•°: {avg_quality:.2f}")
            print(f"  æœ‰è´¨é‡è¯„åˆ†: {len(quality_papers)} ç¯‡")


def main():
    """ä¸»å‡½æ•°"""
    print("è®ºæ–‡æ•°æ®æ•´åˆå™¨")
    print("=" * 60)
    
    integrator = PaperIntegrator()
    
    try:
        # 1. åŠ è½½ç°æœ‰æ•°æ®
        if not integrator.load_existing_papers():
            print("åŠ è½½ç°æœ‰æ•°æ®å¤±è´¥")
            return
        
        # 2. åŠ è½½æ–°æ”¶é›†çš„æ•°æ®
        if not integrator.load_new_papers():
            print("åŠ è½½æ–°æ•°æ®å¤±è´¥ï¼Œè¯·å…ˆè¿è¡Œæ”¶é›†ç¨‹åº")
            return
        
        # 3. æ•´åˆæ•°æ®
        integrated_papers = integrator.integrate_papers()
        
        if integrated_papers:
            # 4. ä¿å­˜æ•´åˆç»“æœ
            output_file = integrator.save_integrated_papers()
            
            print(f"\nâœ… æ•°æ®æ•´åˆå®Œæˆï¼")
            print(f"\nä¸‹ä¸€æ­¥:")
            print(f"1. æŸ¥çœ‹æ•´åˆç»“æœ: {output_file}")
            print(f"2. æµ‹è¯•æ–°ç³»ç»Ÿ: python main_rag_system.py")
            print(f"3. æˆ–è¿è¡Œæµ‹è¯•: python test_direct_answer.py")
        else:
            print("âŒ æ•´åˆå¤±è´¥ï¼Œæ²¡æœ‰æœ‰æ•ˆçš„è®ºæ–‡æ•°æ®")
    
    except Exception as e:
        print(f"âŒ æ•´åˆè¿‡ç¨‹å‡ºç°é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()