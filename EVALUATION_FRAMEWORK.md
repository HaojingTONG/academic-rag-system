# RAGç³»ç»Ÿè¯„ä¼°æ¡†æ¶ (Evaluation Framework)

## ğŸ“Š æ¦‚è¿°

æœ¬è¯„ä¼°æ¡†æ¶ä¸ºå­¦æœ¯è®ºæ–‡RAGç³»ç»Ÿæä¾›äº†å®Œæ•´çš„æ€§èƒ½è¯„ä¼°è§£å†³æ–¹æ¡ˆï¼Œä»å¤šä¸ªç»´åº¦é‡åŒ–RAGç³»ç»Ÿçš„è¡¨ç°ï¼Œå¸®åŠ©è¯†åˆ«æ”¹è¿›æ–¹å‘ã€‚

## ğŸ¯ æ ¸å¿ƒè¯„ä¼°æŒ‡æ ‡

### 1. **ä¸Šä¸‹æ–‡ç›¸å…³æ€§ (Context Relevance)**
- **å®šä¹‰**: æ£€ç´¢åˆ°çš„ä¸Šä¸‹æ–‡ä¸ç”¨æˆ·é—®é¢˜çš„ç›¸å…³ç¨‹åº¦
- **è®¡ç®—æ–¹æ³•**: ä½¿ç”¨è¯­ä¹‰ç›¸ä¼¼åº¦è®¡ç®—é—®é¢˜ä¸ä¸Šä¸‹æ–‡çš„åŒ¹é…åº¦
- **è¯„åˆ†èŒƒå›´**: 0-1ï¼ˆ1åˆ†ä¸ºå®Œå…¨ç›¸å…³ï¼‰

### 2. **ç­”æ¡ˆå¿ å®åº¦ (Faithfulness)**
- **å®šä¹‰**: ç”Ÿæˆçš„ç­”æ¡ˆæ˜¯å¦å®Œå…¨åŸºäºæä¾›çš„ä¸Šä¸‹æ–‡ï¼Œæ²¡æœ‰è™šæ„ä¿¡æ¯
- **è®¡ç®—æ–¹æ³•**: ç»“åˆè¯­ä¹‰ç›¸ä¼¼åº¦å’Œè¯æ±‡é‡å åº¦åˆ†æ
- **è¯„åˆ†èŒƒå›´**: 0-1ï¼ˆ1åˆ†ä¸ºå®Œå…¨å¿ å®ï¼‰

### 3. **ç­”æ¡ˆç›¸å…³æ€§ (Answer Relevance)**
- **å®šä¹‰**: ç­”æ¡ˆæ˜¯å¦ç›´æ¥å›ç­”äº†ç”¨æˆ·çš„é—®é¢˜
- **è®¡ç®—æ–¹æ³•**: è®¡ç®—é—®é¢˜ä¸ç­”æ¡ˆçš„è¯­ä¹‰ç›¸ä¼¼åº¦
- **è¯„åˆ†èŒƒå›´**: 0-1ï¼ˆ1åˆ†ä¸ºå®Œå…¨ç›¸å…³ï¼‰

### 4. **ä¸Šä¸‹æ–‡ç²¾ç¡®åº¦ (Context Precision)**
- **å®šä¹‰**: æ£€ç´¢åˆ°çš„ä¸Šä¸‹æ–‡ä¸­æœ‰å¤šå°‘æ˜¯çœŸæ­£ç›¸å…³çš„
- **è®¡ç®—æ–¹æ³•**: ç›¸å…³ä¸Šä¸‹æ–‡æ•°é‡ / æ€»æ£€ç´¢ä¸Šä¸‹æ–‡æ•°é‡
- **è¯„åˆ†èŒƒå›´**: 0-1ï¼ˆ1åˆ†ä¸ºå®Œå…¨ç²¾ç¡®ï¼‰

### 5. **ä¸Šä¸‹æ–‡å¬å›ç‡ (Context Recall)**
- **å®šä¹‰**: ç›¸å…³æ–‡æ¡£ä¸­æœ‰å¤šå°‘è¢«æˆåŠŸæ£€ç´¢åˆ°
- **è®¡ç®—æ–¹æ³•**: æ£€ç´¢åˆ°çš„ç›¸å…³æ–‡æ¡£ / æ€»ç›¸å…³æ–‡æ¡£ï¼ˆéœ€è¦æ ‡å‡†ç­”æ¡ˆï¼‰
- **è¯„åˆ†èŒƒå›´**: 0-1ï¼ˆ1åˆ†ä¸ºå®Œå…¨å¬å›ï¼‰

## ğŸ—ï¸ æ¡†æ¶æ¶æ„

```
src/evaluation/
â”œâ”€â”€ __init__.py                 # æ¨¡å—åˆå§‹åŒ–
â”œâ”€â”€ evaluation_metrics.py      # æ ¸å¿ƒè¯„ä¼°æŒ‡æ ‡å®ç°
â”œâ”€â”€ benchmark_datasets.py      # åŸºå‡†æ•°æ®é›†ç®¡ç†
â””â”€â”€ rag_evaluator.py           # å®Œæ•´è¯„ä¼°å™¨
```

## ğŸš€ å¿«é€Ÿå¼€å§‹

### 1. åŸºç¡€è¯„ä¼°

```python
from src.evaluation import RAGEvaluator, BenchmarkDatasets

# åˆå§‹åŒ–è¯„ä¼°å™¨
evaluator = RAGEvaluator()

# åˆ›å»ºæµ‹è¯•æ•°æ®é›†
dataset_manager = BenchmarkDatasets()
test_dataset = dataset_manager.create_sample_dataset()

# è¯„ä¼°RAGç³»ç»Ÿ
results = evaluator.evaluate_rag_system(
    rag_system=your_rag_system,
    evaluation_dataset=test_dataset
)
```

### 2. ä½¿ç”¨è¯„ä¼°è„šæœ¬

```bash
# å®Œæ•´è¯„ä¼°
python evaluate_rag_system.py

# å¿«é€Ÿè¯„ä¼°
python evaluate_rag_system.py --quick
```

## ğŸ“‹ è¯„ä¼°æ•°æ®é›†

### æ•°æ®é›†ç±»å‹

1. **ç¤ºä¾‹æ•°æ®é›†**: é¢„å®šä¹‰çš„æµ‹è¯•æ¡ˆä¾‹ï¼Œé€‚åˆå¿«é€Ÿæµ‹è¯•
2. **ç”Ÿæˆæ•°æ®é›†**: åŸºäºè®ºæ–‡å†…å®¹è‡ªåŠ¨ç”Ÿæˆé—®ç­”å¯¹
3. **è‡ªå®šä¹‰æ•°æ®é›†**: æ‰‹åŠ¨åˆ›å»ºçš„ä¸“é—¨æµ‹è¯•æ¡ˆä¾‹

### æ•°æ®é›†ç»“æ„

```python
@dataclass
class EvaluationCase:
    id: str                     # æ¡ˆä¾‹ID
    question: str               # é—®é¢˜
    contexts: List[str]         # æ£€ç´¢åˆ°çš„ä¸Šä¸‹æ–‡
    ground_truth_answer: str    # æ ‡å‡†ç­”æ¡ˆ
    relevant_contexts: List[str] # ç›¸å…³ä¸Šä¸‹æ–‡ï¼ˆç”¨äºå¬å›ç‡ï¼‰
    difficulty: str             # éš¾åº¦ç­‰çº§: easy, medium, hard
    category: str               # é—®é¢˜ç±»åˆ«
    metadata: Dict              # é¢å¤–å…ƒæ•°æ®
```

## ğŸ“Š è¯„ä¼°æŠ¥å‘Š

è¯„ä¼°å®Œæˆåä¼šç”Ÿæˆå¤šç§æ ¼å¼çš„æŠ¥å‘Šï¼š

### 1. JSONæŠ¥å‘Š
- **è¯¦ç»†ç»“æœ**: `evaluation_detailed_YYYYMMDD_HHMMSS.json`
- **æ‘˜è¦æŠ¥å‘Š**: `evaluation_summary_YYYYMMDD_HHMMSS.json`

### 2. CSVæŠ¥å‘Š
- **è¡¨æ ¼æ•°æ®**: `evaluation_report_YYYYMMDD_HHMMSS.csv`
- ä¾¿äºExcelåˆ†æå’Œå¤„ç†

### 3. å¯è§†åŒ–å›¾è¡¨
- **æ€§èƒ½å›¾è¡¨**: `evaluation_charts_YYYYMMDD_HHMMSS.png`
- åŒ…å«åˆ†æ•°åˆ†å¸ƒã€æŒ‡æ ‡å¯¹æ¯”ã€ç›¸å…³æ€§åˆ†æç­‰

## ğŸ”§ é«˜çº§åŠŸèƒ½

### 1. LLMæ·±åº¦è¯„ä¼°

å¯ç”¨LLMè¿›è¡Œæ›´æ·±å…¥çš„è´¨é‡è¯„ä¼°ï¼š

```python
# å¯ç”¨LLMè¯„ä¼°
results = evaluator.evaluate_rag_system(
    rag_system=rag_system,
    evaluation_dataset=dataset,
    use_llm_evaluation=True  # å¯ç”¨LLMè¯„ä¼°
)
```

LLMè¯„ä¼°ç»´åº¦ï¼š
- ä¸Šä¸‹æ–‡ç›¸å…³æ€§
- ç­”æ¡ˆå¿ å®åº¦  
- ç­”æ¡ˆç›¸å…³æ€§
- ç­”æ¡ˆå®Œæ•´æ€§
- ç­”æ¡ˆæ¸…æ™°åº¦

### 2. è‡ªå®šä¹‰è¯„ä¼°æŒ‡æ ‡

```python
class CustomEvaluationMetrics(EvaluationMetrics):
    def custom_metric(self, question: str, answer: str) -> float:
        # å®ç°è‡ªå®šä¹‰è¯„ä¼°é€»è¾‘
        return custom_score

evaluator = RAGEvaluator(custom_metrics=CustomEvaluationMetrics())
```

### 3. æ‰¹é‡è¯„ä¼°

```python
# è¯„ä¼°å¤šä¸ªRAGç³»ç»Ÿé…ç½®
configurations = [
    {"embedding_model": "all-MiniLM-L6-v2", "chunk_size": 500},
    {"embedding_model": "all-mpnet-base-v2", "chunk_size": 1000}
]

for config in configurations:
    rag_system = AcademicRAGSystem(**config)
    results = evaluator.evaluate_rag_system(rag_system, dataset)
```

## ğŸ“ˆ è¯„ä¼°è§£è¯»

### ç»¼åˆè¯„åˆ†æ ‡å‡†

- **0.8-1.0**: ä¼˜ç§€ ğŸ† - ç³»ç»Ÿè¡¨ç°å‡ºè‰²
- **0.7-0.8**: è‰¯å¥½ ğŸ‘ - ç³»ç»Ÿè¡¨ç°è‰¯å¥½
- **0.6-0.7**: ä¸­ç­‰ ğŸ‘Œ - ç³»ç»Ÿè¡¨ç°ä¸€èˆ¬
- **0.5-0.6**: åŠæ ¼ ğŸ“ - ç³»ç»ŸåŸºæœ¬å¯ç”¨
- **0.0-0.5**: éœ€è¦æ”¹è¿› ğŸ“‰ - ç³»ç»Ÿéœ€è¦ä¼˜åŒ–

### æŒ‡æ ‡åˆ†ææŒ‡å—

1. **ä¸Šä¸‹æ–‡ç›¸å…³æ€§ä½** â†’ ä¼˜åŒ–æ£€ç´¢ç®—æ³•ã€æ”¹è¿›æŸ¥è¯¢ç†è§£
2. **ç­”æ¡ˆå¿ å®åº¦ä½** â†’ æ”¹è¿›æç¤ºå·¥ç¨‹ã€å¢å¼ºä¸Šä¸‹æ–‡åˆ©ç”¨
3. **ç­”æ¡ˆç›¸å…³æ€§ä½** â†’ ä¼˜åŒ–é—®ç­”åŒ¹é…ã€æ”¹è¿›ç”Ÿæˆç­–ç•¥
4. **ä¸Šä¸‹æ–‡ç²¾ç¡®åº¦ä½** â†’ æé«˜æ£€ç´¢ç²¾ç¡®åº¦ã€è¿‡æ»¤æ— å…³å†…å®¹
5. **ä¸Šä¸‹æ–‡å¬å›ç‡ä½** â†’ æ‰©å±•æ£€ç´¢èŒƒå›´ã€æ”¹è¿›ç´¢å¼•ç­–ç•¥

## ğŸ”„ æŒç»­æ”¹è¿›æµç¨‹

1. **åŸºçº¿è¯„ä¼°**: å»ºç«‹å½“å‰ç³»ç»Ÿæ€§èƒ½åŸºçº¿
2. **é—®é¢˜è¯†åˆ«**: åˆ†æè–„å¼±ç¯èŠ‚å’Œæ”¹è¿›æœºä¼š
3. **ç³»ç»Ÿä¼˜åŒ–**: é’ˆå¯¹æ€§æ”¹è¿›RAGç³»ç»Ÿç»„ä»¶
4. **æ•ˆæœéªŒè¯**: é‡æ–°è¯„ä¼°éªŒè¯æ”¹è¿›æ•ˆæœ
5. **è¿­ä»£ä¼˜åŒ–**: é‡å¤ä¸Šè¿°æµç¨‹æŒç»­æ”¹è¿›

## ğŸ› ï¸ APIå‚è€ƒ

### EvaluationMetricsç±»

```python
class EvaluationMetrics:
    def __init__(self, embedding_model: str = "all-MiniLM-L6-v2")
    
    def evaluate_single_case(self, 
                           question: str,
                           retrieved_contexts: List[str],
                           generated_answer: str,
                           ground_truth: Optional[str] = None,
                           relevant_contexts: Optional[List[str]] = None
                          ) -> EvaluationResult
```

### BenchmarkDatasetsç±»

```python
class BenchmarkDatasets:
    def __init__(self, data_dir: str = "data/evaluation")
    
    def generate_academic_qa_dataset(self, 
                                   papers_data_path: str, 
                                   num_questions: int = 100
                                  ) -> List[EvaluationCase]
    
    def create_custom_dataset(self, cases: List[Dict]) -> List[EvaluationCase]
    
    def save_dataset(self, dataset: List[EvaluationCase], filename: str)
    
    def load_dataset(self, filename: str) -> List[EvaluationCase]
```

### RAGEvaluatorç±»

```python
class RAGEvaluator:
    def __init__(self, 
                 embedding_model: str = "all-MiniLM-L6-v2",
                 llm_base_url: str = "http://localhost:11434",
                 llm_model: str = "llama3.1:8b",
                 output_dir: str = "data/evaluation/results")
    
    def evaluate_rag_system(self, 
                           rag_system,
                           evaluation_dataset: List[EvaluationCase],
                           use_llm_evaluation: bool = True,
                           save_results: bool = True
                          ) -> Dict[str, Any]
```

## ğŸ“ æœ€ä½³å®è·µ

1. **å®šæœŸè¯„ä¼°**: å»ºè®®å®šæœŸï¼ˆå¦‚æ¯æœˆï¼‰è¯„ä¼°ç³»ç»Ÿæ€§èƒ½
2. **å¤šæ ·åŒ–æµ‹è¯•**: ä½¿ç”¨ä¸åŒéš¾åº¦å’Œç±»å‹çš„æµ‹è¯•æ¡ˆä¾‹
3. **A/Bæµ‹è¯•**: å¯¹æ¯”ä¸åŒé…ç½®çš„æ€§èƒ½å·®å¼‚
4. **ç”¨æˆ·åé¦ˆ**: ç»“åˆå®é™…ç”¨æˆ·åé¦ˆè¿›è¡Œè¯„ä¼°
5. **æŒç»­ç›‘æ§**: åœ¨ç”Ÿäº§ç¯å¢ƒä¸­æŒç»­ç›‘æ§å…³é”®æŒ‡æ ‡

## ğŸ” æ•…éšœæ’é™¤

### å¸¸è§é—®é¢˜

1. **è¯„ä¼°é€Ÿåº¦æ…¢**: ç¦ç”¨LLMè¯„ä¼°æˆ–ä½¿ç”¨æ›´å¿«çš„åµŒå…¥æ¨¡å‹
2. **å†…å­˜ä¸è¶³**: å‡å°‘æ‰¹æ¬¡å¤§å°æˆ–ä½¿ç”¨æ›´å°çš„æ•°æ®é›†  
3. **LLMè¿æ¥å¤±è´¥**: æ£€æŸ¥OllamaæœåŠ¡çŠ¶æ€
4. **ä¾èµ–ç¼ºå¤±**: å®‰è£…matplotlibå’Œseaborn

### æ€§èƒ½ä¼˜åŒ–

```python
# ä½¿ç”¨æ›´å¿«çš„åµŒå…¥æ¨¡å‹
evaluator = RAGEvaluator(embedding_model="all-MiniLM-L6-v2")

# ç¦ç”¨LLMè¯„ä¼°ä»¥æé«˜é€Ÿåº¦
results = evaluator.evaluate_rag_system(
    rag_system=rag_system,
    evaluation_dataset=dataset,
    use_llm_evaluation=False
)
```

## ğŸ“š å‚è€ƒèµ„æº

- **RAGAs Framework**: https://github.com/explodinggradients/ragas
- **TruLens**: https://github.com/truera/trulens
- **Sentence Transformers**: https://www.sbert.net/
- **ç›¸å…³è®ºæ–‡**: 
  - "RAGAS: Automated Evaluation of Retrieval Augmented Generation"
  - "Evaluating Retrieval-Augmented Generation Systems"

---

é€šè¿‡è¿™ä¸ªè¯„ä¼°æ¡†æ¶ï¼Œä½ å¯ä»¥ç§‘å­¦åœ°é‡åŒ–RAGç³»ç»Ÿçš„æ€§èƒ½ï¼Œè¯†åˆ«æ”¹è¿›æ–¹å‘ï¼Œå¹¶è·Ÿè¸ªä¼˜åŒ–æ•ˆæœã€‚å»ºè®®ä»ç¤ºä¾‹æ•°æ®é›†å¼€å§‹ï¼Œé€æ­¥æ‰©å±•åˆ°æ›´å¤æ‚çš„è¯„ä¼°åœºæ™¯ã€‚