#!/usr/bin/env python3
"""
ä¸»RAGç³»ç»Ÿ - åŸºäºtrace_demo.pyçš„æˆåŠŸæ¨¡å¼é‡å†™
ä¸“æ³¨äºç”Ÿæˆé«˜è´¨é‡çš„ä¸­æ–‡å­¦æœ¯å›ç­”
"""

import json
import sys
import time
from pathlib import Path
from typing import Dict, List, Optional

# æ·»åŠ srcåˆ°è·¯å¾„
sys.path.append('src')

class MainRAGSystem:
    """ä¸»RAGç³»ç»Ÿ - ç®€åŒ–ç‰ˆï¼Œä¸“æ³¨äºé«˜è´¨é‡å›ç­”ç”Ÿæˆ"""
    
    def __init__(self):
        """åˆå§‹åŒ–ç³»ç»Ÿ"""
        self.vector_store = None
        self.hybrid_retriever = None
        self.llm_manager = None
        self.papers_loaded = False
        
        print("ğŸš€ ä¸»RAGç³»ç»Ÿåˆå§‹åŒ–...")
        
    def setup_system(self):
        """è®¾ç½®å®Œæ•´ç³»ç»Ÿ"""
        print("\nğŸ“š å¼€å§‹è®¾ç½®RAGç³»ç»Ÿ...")
        
        try:
            # 1. åˆå§‹åŒ–å‘é‡å­˜å‚¨
            if not self._initialize_vector_store():
                return False
            
            # 2. å¤„ç†è®ºæ–‡æ•°æ®
            if not self._load_papers():
                return False
            
            # 3. åˆå§‹åŒ–æ··åˆæ£€ç´¢å™¨
            if not self._initialize_hybrid_retriever():
                return False
            
            # 4. åˆå§‹åŒ–LLMç®¡ç†å™¨
            if not self._initialize_llm_manager():
                return False
            
            print(f"\nâœ… ç³»ç»Ÿè®¾ç½®å®Œæˆï¼")
            print(f"ğŸ“Š å¤„ç†è®ºæ–‡: {len(self.processed_papers)} ç¯‡")
            print(f"ğŸ” æ··åˆæ£€ç´¢: å·²å¯ç”¨")
            print(f"ğŸ¤– LLMæ¨¡å‹: llama3.1:8b")
            
            self.papers_loaded = True
            return True
            
        except Exception as e:
            print(f"âŒ ç³»ç»Ÿè®¾ç½®å¤±è´¥: {e}")
            return False
    
    def _initialize_vector_store(self):
        """åˆå§‹åŒ–å¢å¼ºç‰ˆå‘é‡å­˜å‚¨ - ç¡®ä¿åµŒå…¥æ¨¡å‹ä¸€è‡´æ€§"""
        try:
            from src.retriever.enhanced_vector_store import EnhancedVectorStore
            from src.config.embedding_config import SystemEmbeddingConfig
            
            # ä½¿ç”¨ç³»ç»Ÿç»Ÿä¸€åµŒå…¥é…ç½® - ç¡®ä¿å‘é‡ç©ºé—´ä¸€è‡´æ€§
            embedding_config = SystemEmbeddingConfig.get_default_config()
            
            print(f"ğŸ”§ ä½¿ç”¨ç³»ç»Ÿç»Ÿä¸€åµŒå…¥é…ç½®:")
            print(f"   - æ¨¡å‹: {embedding_config.model_name}")
            print(f"   - ç»´åº¦: {SystemEmbeddingConfig.DEFAULT_DIMENSION}")
            print(f"   - å½’ä¸€åŒ–: {embedding_config.normalize_embeddings}")
            print(f"   - ç¼“å­˜: {embedding_config.cache_enabled}")
            
            self.vector_store = EnhancedVectorStore(
                persist_directory="vector_db",
                embedding_config=embedding_config
            )
            print("âœ… å¢å¼ºç‰ˆå‘é‡å­˜å‚¨åˆå§‹åŒ–æˆåŠŸ")
            return True
        except Exception as e:
            print(f"âŒ å¢å¼ºç‰ˆå‘é‡å­˜å‚¨åˆå§‹åŒ–å¤±è´¥: {e}")
            print("âš ï¸ å›é€€åˆ°åŸºç¡€å‘é‡å­˜å‚¨ï¼ˆä½¿ç”¨ç›¸åŒåµŒå…¥æ¨¡å‹ç¡®ä¿ä¸€è‡´æ€§ï¼‰...")
            try:
                from src.retriever.vector_store import VectorStore
                from src.config.embedding_config import SystemEmbeddingConfig
                
                # å³ä½¿åœ¨å›é€€æƒ…å†µä¸‹ï¼Œä¹Ÿå¿…é¡»ä½¿ç”¨ç›¸åŒçš„åµŒå…¥æ¨¡å‹
                # æ£€æŸ¥åŸºç¡€å‘é‡å­˜å‚¨æ˜¯å¦ä½¿ç”¨ç›¸åŒæ¨¡å‹
                if hasattr(VectorStore, '__init__'):
                    # åˆ›å»ºåŸºç¡€å‘é‡å­˜å‚¨ï¼Œä½†ç¡®ä¿ä½¿ç”¨ç›¸åŒåµŒå…¥æ¨¡å‹
                    self.vector_store = VectorStore()
                    
                    # éªŒè¯åŸºç¡€å‘é‡å­˜å‚¨çš„åµŒå…¥æ¨¡å‹æ˜¯å¦ä¸€è‡´
                    expected_model = SystemEmbeddingConfig.DEFAULT_MODEL_NAME
                    if hasattr(self.vector_store, 'encoder'):
                        actual_model = getattr(self.vector_store.encoder, 'model_name', 'unknown')
                        if 'all-MiniLM-L6-v2' in str(actual_model):
                            print("âŒ åŸºç¡€å‘é‡å­˜å‚¨ä½¿ç”¨äº†ä¸åŒçš„åµŒå…¥æ¨¡å‹!")
                            print(f"   æœŸæœ›: {expected_model}")
                            print(f"   å®é™…: {actual_model}")
                            print("ğŸš¨ å‘é‡ç©ºé—´ä¸ä¸€è‡´ï¼Œæ— æ³•å®‰å…¨å›é€€!")
                            return False
                    
                    print("âœ… åŸºç¡€å‘é‡å­˜å‚¨åˆå§‹åŒ–æˆåŠŸï¼ˆåµŒå…¥æ¨¡å‹ä¸€è‡´ï¼‰")
                    return True
                else:
                    return False
            except Exception as e2:
                print(f"âŒ åŸºç¡€å‘é‡å­˜å‚¨ä¹Ÿå¤±è´¥: {e2}")
                return False
    
    def _load_papers(self):
        """åŠ è½½è®ºæ–‡æ•°æ®å¹¶è¿›è¡Œæ™ºèƒ½åˆ‡åˆ†"""
        try:
            papers_file = "data/papers_info.json"
            if not Path(papers_file).exists():
                print(f"âŒ è®ºæ–‡æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {papers_file}")
                return False
            
            with open(papers_file, 'r', encoding='utf-8') as f:
                self.papers_data = json.load(f)
            
            print(f"ğŸ“„ åŠ è½½è®ºæ–‡æ•°æ®: {len(self.papers_data)} ç¯‡")
            
            # åˆå§‹åŒ–æ–‡æ¡£åˆ‡åˆ†å™¨
            from src.processor.document_chunker import DocumentChunker, ChunkingConfig
            
            chunking_config = ChunkingConfig(
                strategy="hybrid",           # ä½¿ç”¨æ··åˆç­–ç•¥
                chunk_size=600,             # å—å¤§å°600å­—ç¬¦
                chunk_overlap=100,          # é‡å 100å­—ç¬¦
                min_chunk_size=150,         # æœ€å°å—å¤§å°
                max_chunk_size=1200,        # æœ€å¤§å—å¤§å°
                preserve_paragraphs=True,   # ä¿æŒæ®µè½å®Œæ•´
                preserve_sentences=True,    # ä¿æŒå¥å­å®Œæ•´
                section_aware=True          # æ„ŸçŸ¥ç« èŠ‚ç»“æ„
            )
            
            self.document_chunker = DocumentChunker(chunking_config)
            print(f"ğŸ”ª æ–‡æ¡£åˆ‡åˆ†é…ç½®: {chunking_config.strategy}ç­–ç•¥, å—å¤§å°{chunking_config.chunk_size}, é‡å {chunking_config.chunk_overlap}")
            
            # å¤„ç†è®ºæ–‡å¹¶è¿›è¡Œæ™ºèƒ½åˆ‡åˆ†
            self.processed_papers = []
            all_chunks = []
            documents = []
            metadatas = []
            
            total_chunks = 0
            
            for i, paper in enumerate(self.papers_data):
                title = paper.get('title', '')
                abstract = paper.get('abstract', '')
                
                if len(title) < 10 or len(abstract) < 50:
                    continue
                
                # åˆ›å»ºå®Œæ•´æ–‡æ¡£å†…å®¹
                content = f"Title: {title}\n\nAbstract: {abstract}"
                
                # åŸºç¡€å…ƒæ•°æ®
                base_metadata = {
                    'paper_id': paper['id'],
                    'title': title,
                    'authors': paper.get('authors', []),
                    'published': paper.get('published', ''),
                    'pdf_url': paper.get('pdf_url', ''),
                    'source_paper': paper
                }
                
                # ä½¿ç”¨æ–‡æ¡£åˆ‡åˆ†å™¨è¿›è¡Œæ™ºèƒ½åˆ‡åˆ†
                chunks = self.document_chunker.chunk_document(content, paper['id'], base_metadata)
                
                if chunks:
                    # æ”¶é›†æ‰€æœ‰chunks
                    all_chunks.extend(chunks)
                    total_chunks += len(chunks)
                    
                    # è½¬æ¢ä¸ºå‘é‡å­˜å‚¨æ ¼å¼
                    for chunk in chunks:
                        documents.append(chunk.text)
                        metadatas.append(chunk.metadata)
                    
                    # ä¿å­˜å¤„ç†åçš„è®ºæ–‡ä¿¡æ¯
                    paper_copy = paper.copy()
                    paper_copy['chunks'] = [
                        {
                            'chunk_id': chunk.chunk_id,
                            'text': chunk.text,
                            'section_type': chunk.section_type,
                            'word_count': chunk.word_count,
                            'char_count': chunk.char_count
                        }
                        for chunk in chunks
                    ]
                    paper_copy['chunk_count'] = len(chunks)
                    self.processed_papers.append(paper_copy)
                
                if (i + 1) % 10 == 0:
                    print(f"  å·²å¤„ç† {i + 1}/{len(self.papers_data)} ç¯‡è®ºæ–‡...")
            
            # è·å–åˆ‡åˆ†ç»Ÿè®¡ä¿¡æ¯
            chunking_stats = self.document_chunker.get_chunking_stats(all_chunks)
            
            print(f"\nğŸ“Š æ–‡æ¡£åˆ‡åˆ†ç»Ÿè®¡:")
            print(f"  æ€»æ–‡æ¡£å—æ•°: {chunking_stats['total_chunks']}")
            print(f"  å¹³å‡å—å¤§å°: {chunking_stats['avg_chunk_size']:.1f} å­—ç¬¦")
            print(f"  å¹³å‡è¯æ•°: {chunking_stats['avg_word_count']:.1f}")
            print(f"  å—å¤§å°èŒƒå›´: {chunking_stats['min_chunk_size']} - {chunking_stats['max_chunk_size']} å­—ç¬¦")
            print(f"  è¯†åˆ«ç« èŠ‚ç±»å‹: {', '.join(chunking_stats['section_types'])}")
            
            # æ„å»ºå‘é‡æ•°æ®åº“
            print("\nğŸ”¢ æ„å»ºå‘é‡æ•°æ®åº“...")
            self.vector_store.add_papers_with_metadata(documents, metadatas)
            self.documents = documents
            self.all_chunks = all_chunks
            
            print(f"âœ… å¤„ç†å®Œæˆ: {len(self.processed_papers)} ç¯‡è®ºæ–‡, {total_chunks} ä¸ªæ™ºèƒ½æ–‡æ¡£å—")
            return True
            
        except Exception as e:
            print(f"âŒ è®ºæ–‡æ•°æ®åŠ è½½å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return False
    
    def _initialize_hybrid_retriever(self):
        """åˆå§‹åŒ–æ··åˆæ£€ç´¢å™¨"""
        try:
            from src.retriever.advanced_retrieval import HybridRetriever
            self.hybrid_retriever = HybridRetriever(self.vector_store)
            self.hybrid_retriever.fit(self.documents)
            print("âœ… æ··åˆæ£€ç´¢å™¨åˆå§‹åŒ–æˆåŠŸ")
            return True
        except Exception as e:
            print(f"âŒ æ··åˆæ£€ç´¢å™¨åˆå§‹åŒ–å¤±è´¥: {e}")
            return False
    
    def _initialize_llm_manager(self):
        """åˆå§‹åŒ–LLMç®¡ç†å™¨"""
        try:
            from src.generator.llm_client import get_llm_manager
            self.llm_manager = get_llm_manager("llama3.1:8b")
            print("âœ… LLMç®¡ç†å™¨åˆå§‹åŒ–æˆåŠŸ")
            return True
        except Exception as e:
            print(f"âŒ LLMç®¡ç†å™¨åˆå§‹åŒ–å¤±è´¥: {e}")
            return False
    
    def query(self, question: str) -> Dict:
        """å¤„ç†ç”¨æˆ·æŸ¥è¯¢å¹¶ç”Ÿæˆé«˜è´¨é‡å›ç­”"""
        if not self.papers_loaded:
            return {"error": "ç³»ç»Ÿæœªå°±ç»ªï¼Œè¯·å…ˆè¿è¡Œsetup_system()"}
        
        start_time = time.time()
        print(f"\nğŸ” å¤„ç†æŸ¥è¯¢: {question}")
        
        try:
            # 1. æ‰§è¡Œå¢å¼ºå‘é‡æ£€ç´¢
            print("  ğŸ¯ æ‰§è¡Œå¢å¼ºå‘é‡æ£€ç´¢ (å¸¦ç›¸ä¼¼åº¦é˜ˆå€¼è¿‡æ»¤)...")
            enhanced_results = self.vector_store.advanced_search(
                query=question,
                top_k=8,
                similarity_threshold=0.3,
                enable_adaptive_k=True,
                enable_diversity=False
            )
            
            # 2. æ‰§è¡Œæ··åˆæ£€ç´¢ä½œä¸ºè¡¥å……
            print("  ğŸ“– æ‰§è¡Œæ··åˆæ£€ç´¢...")
            hybrid_results = self.hybrid_retriever.search(
                question, 
                top_k=5, 
                use_reranking=True, 
                use_diversity=True
            )
            
            # 3. åˆå¹¶å’Œä¼˜åŒ–æ£€ç´¢ç»“æœ
            print(f"  ğŸ“Š å¢å¼ºæ£€ç´¢ç»“æœ: {len(enhanced_results)} ä¸ª")
            
            # è·å–æ··åˆæ£€ç´¢ç»“æœ
            final_results = hybrid_results.get('final_results', hybrid_results.get('results', []))
            print(f"  ğŸ“Š æ··åˆæ£€ç´¢ç»“æœ: {len(final_results)} ä¸ª")
            
            # ä¼˜å…ˆä½¿ç”¨å¢å¼ºå‘é‡æ£€ç´¢ç»“æœï¼Œå¦‚æœç»“æœä¸è¶³åˆ™è¡¥å……æ··åˆæ£€ç´¢ç»“æœ
            if enhanced_results:
                # å°†å¢å¼ºæ£€ç´¢ç»“æœè½¬æ¢ä¸ºæ··åˆæ£€ç´¢æ ¼å¼
                enhanced_docs = []
                for result in enhanced_results:
                    enhanced_docs.append({
                        'content': result.document,
                        'metadata': result.metadata,
                        'similarity_score': result.similarity_score,
                        'source': 'enhanced_vector'
                    })
                
                # å¦‚æœå¢å¼ºæ£€ç´¢ç»“æœå……è¶³ï¼Œç›´æ¥ä½¿ç”¨ï¼›å¦åˆ™è¡¥å……æ··åˆæ£€ç´¢ç»“æœ
                if len(enhanced_docs) >= 3:
                    final_results = enhanced_docs[:5]  # å–å‰5ä¸ªæœ€ä½³ç»“æœ
                else:
                    final_results = enhanced_docs + final_results[:max(0, 5-len(enhanced_docs))]
            
            if not final_results:
                return {
                    "question": question,
                    "answer": "æŠ±æ­‰ï¼Œæœªæ‰¾åˆ°ç›¸å…³çš„å­¦æœ¯èµ„æ–™æ¥å›ç­”æ‚¨çš„é—®é¢˜ã€‚å»ºè®®é™ä½ç›¸ä¼¼åº¦é˜ˆå€¼é‡è¯•ã€‚",
                    "query_time": f"{time.time() - start_time:.2f}ç§’",
                    "retrieval_info": {
                        "enhanced_results": len(enhanced_results),
                        "hybrid_results": len(hybrid_results.get('final_results', []))
                    }
                }
            
            print(f"  âœ… æ£€ç´¢åˆ° {len(final_results)} ä¸ªç›¸å…³æ–‡æ¡£")
            
            # 3. æ„å»ºå¢å¼ºPrompt (ä½¿ç”¨å…ˆè¿›çš„æç¤ºå·¥ç¨‹)
            print("  ğŸ› ï¸ æ„å»ºæ™ºèƒ½æç¤ºè¯...")
            from src.generator.prompt_engineering import PromptBuilder
            
            prompt_builder = PromptBuilder()
            
            # ç›´æ¥ä¼ é€’åŸå§‹æ£€ç´¢ç»“æœç»™æç¤ºå·¥ç¨‹æ¨¡å—
            # æç¤ºå·¥ç¨‹æ¨¡å—ä¼šå†…éƒ¨å¤„ç†å’Œè½¬æ¢æ•°æ®æ ¼å¼
            prompt_result = prompt_builder.build_prompt(
                query=question,
                retrieved_results=final_results,
                max_context_length=4000  # æ§åˆ¶æç¤ºè¯é•¿åº¦
            )
            
            # 4. è°ƒç”¨LLMç”Ÿæˆå›ç­”
            print("  ğŸ¤– ç”Ÿæˆæ™ºèƒ½å›ç­”...")
            
            response = self.llm_manager.generate_answer(
                prompt=prompt_result["prompt"],
                query_intent=prompt_result["query_type"],
                max_tokens=512,
                temperature=0.7
            )
            
            query_time = time.time() - start_time
            
            if response.success and response.text.strip():
                return {
                    "question": question,
                    "answer": response.text.strip(),
                    "sources": [r.get('metadata', {}) if isinstance(r, dict) else r.metadata for r in final_results],
                    "query_time": f"{query_time:.2f}ç§’",
                    "model": response.model,
                    "results_count": len(final_results)
                }
            else:
                return {
                    "question": question,
                    "error": f"LLMç”Ÿæˆå¤±è´¥: {response.error_message}",
                    "query_time": f"{query_time:.2f}ç§’"
                }
                
        except Exception as e:
            return {
                "question": question,
                "error": f"æŸ¥è¯¢å¤„ç†å¤±è´¥: {str(e)}",
                "query_time": f"{time.time() - start_time:.2f}ç§’"
            }
    
    def get_system_status(self):
        """è·å–ç³»ç»ŸçŠ¶æ€"""
        status = {
            "papers_loaded": self.papers_loaded,
            "papers_count": len(self.processed_papers) if hasattr(self, 'processed_papers') else 0,
            "vector_store_ready": self.vector_store is not None,
            "hybrid_retriever_ready": self.hybrid_retriever is not None,
            "llm_manager_ready": self.llm_manager is not None
        }
        
        # æ·»åŠ æ–‡æ¡£åˆ‡åˆ†ç»Ÿè®¡ä¿¡æ¯
        if hasattr(self, 'all_chunks') and self.all_chunks:
            chunking_stats = self.document_chunker.get_chunking_stats(self.all_chunks)
            status.update({
                "total_chunks": chunking_stats['total_chunks'],
                "avg_chunk_size": f"{chunking_stats['avg_chunk_size']:.1f} å­—ç¬¦",
                "chunks_per_paper": f"{chunking_stats['total_chunks'] / len(self.processed_papers):.1f}" if self.processed_papers else "0",
                "chunking_strategy": self.document_chunker.config.strategy,
                "section_types": chunking_stats['section_types']
            })
        
        # æ·»åŠ åµŒå…¥æ¨¡å‹ä¿¡æ¯
        if hasattr(self.vector_store, 'get_collection_stats'):
            try:
                vector_stats = self.vector_store.get_collection_stats()
                status.update({
                    "embedding_model": vector_stats.get("embedding_model", "unknown"),
                    "embedding_dimension": vector_stats.get("embedding_dimension", "unknown"),
                    "embedding_device": vector_stats.get("device", "unknown"),
                    "embedding_cache": "å¯ç”¨" if vector_stats.get("cache_enabled", False) else "ç¦ç”¨"
                })
            except:
                pass
        
        return status


def main():
    """ä¸»ç¨‹åºå…¥å£"""
    print("ğŸ¯ å­¦æœ¯RAGé—®ç­”ç³»ç»Ÿ")
    print("=" * 50)
    
    # åˆå§‹åŒ–ç³»ç»Ÿ
    rag = MainRAGSystem()
    
    # è®¾ç½®ç³»ç»Ÿ
    if not rag.setup_system():
        print("âŒ ç³»ç»Ÿè®¾ç½®å¤±è´¥ï¼Œé€€å‡º")
        return
    
    # æ˜¾ç¤ºä½¿ç”¨è¯´æ˜
    print(f"\nğŸ’¡ ç³»ç»Ÿå°±ç»ªï¼ä½¿ç”¨è¯´æ˜:")
    print(f"  â€¢ ç›´æ¥è¾“å…¥é—®é¢˜è¿›è¡ŒæŸ¥è¯¢")
    print(f"  â€¢ è¾“å…¥ 'status' æŸ¥çœ‹ç³»ç»ŸçŠ¶æ€")
    print(f"  â€¢ è¾“å…¥ 'quit' æˆ– 'exit' é€€å‡ºç³»ç»Ÿ")
    print(f"  â€¢ è¾“å…¥ 'help' æŸ¥çœ‹å¸®åŠ©")
    print("=" * 50)
    
    # äº¤äº’å¼æŸ¥è¯¢å¾ªç¯
    while True:
        try:
            user_input = input("\nâ“ è¯·è¾“å…¥æ‚¨çš„é—®é¢˜: ").strip()
            
            if not user_input:
                continue
            
            # å¤„ç†ç‰¹æ®Šå‘½ä»¤
            if user_input.lower() in ['quit', 'exit', 'q']:
                print("ğŸ‘‹ æ„Ÿè°¢ä½¿ç”¨ï¼Œå†è§ï¼")
                break
            
            elif user_input.lower() == 'status':
                status = rag.get_system_status()
                print(f"\nğŸ“Š ç³»ç»ŸçŠ¶æ€:")
                print(f"  è®ºæ–‡æ•°æ®: {'âœ…' if status['papers_loaded'] else 'âŒ'} ({status['papers_count']} ç¯‡)")
                print(f"  å‘é‡å­˜å‚¨: {'âœ…' if status['vector_store_ready'] else 'âŒ'}")
                print(f"  æ··åˆæ£€ç´¢: {'âœ…' if status['hybrid_retriever_ready'] else 'âŒ'}")
                print(f"  LLMæ¨¡å‹: {'âœ…' if status['llm_manager_ready'] else 'âŒ'}")
                
                # æ˜¾ç¤ºæ–‡æ¡£åˆ‡åˆ†ä¿¡æ¯
                if 'total_chunks' in status:
                    print(f"\nğŸ”ª æ–‡æ¡£åˆ‡åˆ†è¯¦æƒ…:")
                    print(f"  åˆ‡åˆ†ç­–ç•¥: {status['chunking_strategy']}")
                    print(f"  æ€»æ–‡æ¡£å—: {status['total_chunks']} ä¸ª")
                    print(f"  å¹³å‡å—å¤§å°: {status['avg_chunk_size']}")
                    print(f"  æ¯ç¯‡è®ºæ–‡å—æ•°: {status['chunks_per_paper']} ä¸ª")
                    print(f"  è¯†åˆ«ç« èŠ‚ç±»å‹: {', '.join(status['section_types'])}")
                
                # æ˜¾ç¤ºåµŒå…¥æ¨¡å‹ä¿¡æ¯
                if 'embedding_model' in status:
                    print(f"\nğŸ”¢ åµŒå…¥æ¨¡å‹è¯¦æƒ…:")
                    print(f"  æ¨¡å‹åç§°: {status['embedding_model']}")
                    print(f"  å‘é‡ç»´åº¦: {status['embedding_dimension']}")
                    print(f"  è®¡ç®—è®¾å¤‡: {status['embedding_device']}")
                    print(f"  ç¼“å­˜çŠ¶æ€: {status['embedding_cache']}")
                continue
            
            elif user_input.lower() == 'help':
                print(f"\nğŸ“– å¸®åŠ©ä¿¡æ¯:")
                print(f"  æœ¬ç³»ç»Ÿæ˜¯åŸºäºå­¦æœ¯è®ºæ–‡çš„æ™ºèƒ½é—®ç­”ç³»ç»Ÿ")
                print(f"  â€¢ æ”¯æŒä¸­è‹±æ–‡é—®é¢˜")
                print(f"  â€¢ åŸºäºæ··åˆæ£€ç´¢æŠ€æœ¯")
                print(f"  â€¢ ä½¿ç”¨æœ¬åœ°LLMç”Ÿæˆå›ç­”")
                print(f"  â€¢ è‡ªåŠ¨å¼•ç”¨ç›¸å…³å­¦æœ¯èµ„æº")
                print(f"\nğŸ’¡ ç¤ºä¾‹é—®é¢˜:")
                print(f"  - ä»€ä¹ˆæ˜¯transformeræ¶æ„ï¼Ÿ")
                print(f"  - è§£é‡Šæ³¨æ„åŠ›æœºåˆ¶çš„åŸç†")
                print(f"  - æ¯”è¾ƒCNNå’ŒRNNçš„ä¼˜ç¼ºç‚¹")
                continue
            
            # å¤„ç†æŸ¥è¯¢
            print("â³ æ­£åœ¨å¤„ç†æ‚¨çš„é—®é¢˜...")
            result = rag.query(user_input)
            
            if "error" not in result:
                print(f"\nğŸ“ æ™ºèƒ½å›ç­”:")
                print("-" * 50)
                print(result['answer'])
                print("-" * 50)
                print(f"â±ï¸  æŸ¥è¯¢è€—æ—¶: {result['query_time']}")
                print(f"ğŸ“š å‚è€ƒæº: {result['results_count']} ç¯‡è®ºæ–‡")
                print(f"ğŸ¤– æ¨¡å‹: {result.get('model', 'Unknown')}")
            else:
                print(f"\nâŒ æŸ¥è¯¢å¤±è´¥: {result['error']}")
                if 'query_time' in result:
                    print(f"â±ï¸  æŸ¥è¯¢è€—æ—¶: {result['query_time']}")
            
        except KeyboardInterrupt:
            print("\n\nğŸ‘‹ æ„Ÿè°¢ä½¿ç”¨ï¼Œå†è§ï¼")
            break
        except Exception as e:
            print(f"\nâŒ å‘ç”Ÿé”™è¯¯: {e}")


if __name__ == "__main__":
    main()