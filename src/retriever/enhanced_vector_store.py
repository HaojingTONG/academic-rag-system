# src/retriever/enhanced_vector_store.py
"""
å¢å¼ºç‰ˆå‘é‡å­˜å‚¨ - ä½¿ç”¨é«˜çº§åµŒå…¥åŠŸèƒ½
"""

import chromadb
import numpy as np
from typing import List, Dict, Optional, Union
import sys
from pathlib import Path

# æ·»åŠ embeddingæ¨¡å—è·¯å¾„
sys.path.append(str(Path(__file__).parent.parent))
from embedding.advanced_embedding import EmbeddingManager, EmbeddingConfig

class EnhancedVectorStore:
    """å¢å¼ºç‰ˆå‘é‡å­˜å‚¨"""
    
    def __init__(self, 
                 persist_directory="vector_db",
                 embedding_model="all-mpnet-base-v2",  # é»˜è®¤ä½¿ç”¨é«˜è´¨é‡æ¨¡å‹
                 embedding_config: EmbeddingConfig = None):
        """
        åˆå§‹åŒ–å¢å¼ºç‰ˆå‘é‡å­˜å‚¨
        
        Args:
            persist_directory: æŒä¹…åŒ–ç›®å½•
            embedding_model: åµŒå…¥æ¨¡å‹åç§°
            embedding_config: è‡ªå®šä¹‰åµŒå…¥é…ç½®
        """
        
        print(f"ğŸš€ åˆå§‹åŒ–å¢å¼ºç‰ˆå‘é‡å­˜å‚¨...")
        
        # åˆå§‹åŒ–ChromaDB
        self.client = chromadb.PersistentClient(path=persist_directory)
        self.collection = self.client.get_or_create_collection(
            name="enhanced_ai_papers_v2",  # æ–°ç‰ˆæœ¬collection
            metadata={"hnsw:space": "cosine"}
        )
        
        # åˆå§‹åŒ–åµŒå…¥ç®¡ç†å™¨
        if embedding_config is None:
            embedding_config = EmbeddingConfig(
                model_name=embedding_model,
                model_type="sentence_transformers",
                batch_size=16,
                normalize_embeddings=True,
                cache_enabled=True,
                cache_dir="data/embedding_cache"
            )
        
        self.embedding_config = embedding_config
        self.embedding_manager = EmbeddingManager(embedding_config)
        self.embedding_manager.initialize()
        
        # è·å–æ¨¡å‹ä¿¡æ¯
        self.model_info = self.embedding_manager.get_model_info()
        
        print(f"âœ… å‘é‡å­˜å‚¨åˆå§‹åŒ–å®Œæˆ")
        print(f"   - åµŒå…¥æ¨¡å‹: {self.model_info.get('model_name', 'unknown')}")
        print(f"   - å‘é‡ç»´åº¦: {self.model_info.get('dimension', 'unknown')}")
        print(f"   - è®¡ç®—è®¾å¤‡: {self.model_info.get('device', 'unknown')}")
        
    def add_papers_with_metadata(self, documents: List[str], metadatas: List[Dict]):
        """æ·»åŠ å¸¦æœ‰å¢å¼ºå…ƒæ•°æ®çš„æ–‡æ¡£å—"""
        if not documents or not metadatas:
            print("âš ï¸ æ–‡æ¡£æˆ–å…ƒæ•°æ®ä¸ºç©º")
            return
        
        print(f"ğŸ“ æ·»åŠ  {len(documents)} ä¸ªæ–‡æ¡£å—åˆ°å¢å¼ºå‘é‡æ•°æ®åº“...")
        
        # ç”Ÿæˆå”¯ä¸€ID
        ids = []
        for i, metadata in enumerate(metadatas):
            chunk_id = metadata.get('chunk_id', f'chunk_{i}')
            paper_id = metadata.get('paper_id', 'unknown')
            ids.append(f"enhanced_{paper_id}_{chunk_id}")
        
        # ç”ŸæˆåµŒå…¥å‘é‡
        print("ğŸ”¢ ç”Ÿæˆé«˜çº§åµŒå…¥å‘é‡...")
        start_time = __import__('time').time()
        
        embeddings = self.embedding_manager.encode(
            documents, 
            show_progress=True, 
            use_cache=True
        )
        
        encoding_time = __import__('time').time() - start_time
        print(f"â±ï¸ åµŒå…¥ç”Ÿæˆè€—æ—¶: {encoding_time:.2f}ç§’")
        
        # æ‰¹é‡æ·»åŠ åˆ°æ•°æ®åº“
        batch_size = 50  # é€‚ä¸­çš„æ‰¹æ¬¡å¤§å°
        total_added = 0
        
        for i in range(0, len(documents), batch_size):
            end_idx = min(i + batch_size, len(documents))
            
            batch_embeddings = embeddings[i:end_idx]
            batch_documents = documents[i:end_idx]
            batch_metadatas = metadatas[i:end_idx]
            batch_ids = ids[i:end_idx]
            
            # ç¡®ä¿åµŒå…¥æ•°æ®ç±»å‹æ­£ç¡®
            if isinstance(batch_embeddings, np.ndarray):
                batch_embeddings = batch_embeddings.tolist()
            
            try:
                self.collection.add(
                    embeddings=batch_embeddings,
                    documents=batch_documents,
                    metadatas=batch_metadatas,
                    ids=batch_ids
                )
                
                total_added += len(batch_documents)
                print(f"âœ… å·²æ·»åŠ  {total_added}/{len(documents)} ä¸ªæ–‡æ¡£å—")
                
            except Exception as e:
                print(f"âŒ æ‰¹æ¬¡ {i//batch_size + 1} æ·»åŠ å¤±è´¥: {e}")
                continue
        
        # æ˜¾ç¤ºåµŒå…¥ç»Ÿè®¡
        stats = self.embedding_manager.get_stats()
        print(f"\nğŸ“Š åµŒå…¥ç»Ÿè®¡:")
        print(f"   - æ€»ç¼–ç æ•°é‡: {stats['total_encoded']}")
        print(f"   - å¹³å‡ç¼–ç æ—¶é—´: {stats['avg_encoding_time']:.3f}ç§’/æ–‡æœ¬")
        print(f"   - ç¼“å­˜å‘½ä¸­: {stats.get('cache_hits', 0)}")
        
        print("âœ… å¢å¼ºæ–‡æ¡£å—æ·»åŠ å®Œæˆ")
    
    def search(self, query: str, top_k: int = 5, filter_metadata: Dict = None) -> Dict:
        """æœç´¢ç›¸å…³æ–‡æ¡£ï¼ˆä½¿ç”¨é«˜çº§åµŒå…¥ï¼‰"""
        
        # ä½¿ç”¨é«˜çº§åµŒå…¥æ¨¡å‹ç”ŸæˆæŸ¥è¯¢å‘é‡
        query_embedding = self.embedding_manager.encode(query, show_progress=False)
        
        # ç¡®ä¿æŸ¥è¯¢åµŒå…¥æ ¼å¼æ­£ç¡®
        if isinstance(query_embedding, np.ndarray):
            if query_embedding.ndim == 1:
                query_embedding = query_embedding.reshape(1, -1)
            query_embedding = query_embedding.tolist()
        
        # æ„å»ºæœç´¢å‚æ•°
        search_params = {
            "query_embeddings": query_embedding,
            "n_results": top_k,
            "include": ["documents", "metadatas", "distances"]
        }
        
        # æ·»åŠ å…ƒæ•°æ®è¿‡æ»¤
        if filter_metadata:
            search_params["where"] = filter_metadata
        
        # æ‰§è¡Œæœç´¢
        try:
            results = self.collection.query(**search_params)
            
            # æ ¼å¼åŒ–è¿”å›ç»“æœ
            return {
                "documents": results.get("documents", [[]])[0],
                "metadatas": results.get("metadatas", [[]])[0], 
                "distances": results.get("distances", [[]])[0]
            }
            
        except Exception as e:
            print(f"âŒ æœç´¢å¤±è´¥: {e}")
            return {
                "documents": [],
                "metadatas": [],
                "distances": []
            }
    
    def get_collection_stats(self) -> Dict:
        """è·å–é›†åˆç»Ÿè®¡ä¿¡æ¯"""
        try:
            collection_info = self.collection.count()
            
            return {
                "total_documents": collection_info,
                "embedding_model": self.model_info.get("model_name", "unknown"),
                "embedding_dimension": self.model_info.get("dimension", "unknown"),
                "device": self.model_info.get("device", "unknown"),
                "cache_enabled": self.embedding_config.cache_enabled,
                "embedding_stats": self.embedding_manager.get_stats()
            }
        except Exception as e:
            return {"error": str(e)}
    
    def similarity_search_with_scores(self, query: str, top_k: int = 5) -> List[tuple]:
        """ç›¸ä¼¼æ€§æœç´¢å¹¶è¿”å›åˆ†æ•°"""
        results = self.search(query, top_k)
        
        search_results = []
        documents = results.get("documents", [])
        metadatas = results.get("metadatas", [])
        distances = results.get("distances", [])
        
        for doc, metadata, distance in zip(documents, metadatas, distances):
            # å°†è·ç¦»è½¬æ¢ä¸ºç›¸ä¼¼åº¦åˆ†æ•° (1 - cosine_distance)
            similarity_score = 1.0 - distance
            search_results.append((doc, metadata, similarity_score))
        
        return search_results
    
    def advanced_search(self, 
                       query: str,
                       top_k: int = 5,
                       similarity_threshold: float = 0.3,
                       filter_metadata: Dict = None,
                       enable_adaptive_k: bool = True,
                       enable_diversity: bool = False):
        """
        é«˜çº§å‘é‡æ£€ç´¢ - æ”¯æŒç›¸ä¼¼åº¦é˜ˆå€¼è¿‡æ»¤å’Œæ™ºèƒ½Top-Ké€‰æ‹©
        
        Args:
            query: æŸ¥è¯¢æ–‡æœ¬
            top_k: è¿”å›ç»“æœæ•°é‡
            similarity_threshold: ç›¸ä¼¼åº¦é˜ˆå€¼
            filter_metadata: å…ƒæ•°æ®è¿‡æ»¤
            enable_adaptive_k: å¯ç”¨è‡ªé€‚åº”Kå€¼è°ƒæ•´
            enable_diversity: å¯ç”¨ç»“æœå¤šæ ·æ€§
            
        Returns:
            å¢å¼ºæ£€ç´¢ç»“æœ
        """
        from src.retriever.enhanced_vector_retrieval import EnhancedVectorRetriever, RetrievalConfig
        
        # åˆ›å»ºæ£€ç´¢é…ç½®
        config = RetrievalConfig(
            default_top_k=top_k,
            similarity_threshold=similarity_threshold,
            enable_adaptive_k=enable_adaptive_k,
            enable_diversity=enable_diversity
        )
        
        # åˆ›å»ºå¢å¼ºæ£€ç´¢å™¨
        retriever = EnhancedVectorRetriever(self, config)
        
        # æ‰§è¡Œæ£€ç´¢
        return retriever.search(
            query=query,
            top_k=top_k,
            similarity_threshold=similarity_threshold,
            filter_metadata=filter_metadata,
            enable_adaptive_k=enable_adaptive_k,
            enable_diversity=enable_diversity
        )
    
    def delete_collection(self):
        """åˆ é™¤æ•´ä¸ªé›†åˆ"""
        try:
            self.client.delete_collection(name="enhanced_ai_papers_v2")
            print("âœ… é›†åˆåˆ é™¤æˆåŠŸ")
        except Exception as e:
            print(f"âŒ é›†åˆåˆ é™¤å¤±è´¥: {e}")
    
    def rebuild_embeddings(self, new_model_name: str):
        """ä½¿ç”¨æ–°æ¨¡å‹é‡å»ºåµŒå…¥"""
        print(f"ğŸ”„ å‡†å¤‡ä½¿ç”¨æ–°æ¨¡å‹é‡å»ºåµŒå…¥: {new_model_name}")
        
        # è·å–ç°æœ‰æ–‡æ¡£
        try:
            all_results = self.collection.get()
            documents = all_results.get("documents", [])
            metadatas = all_results.get("metadatas", [])
            
            if not documents:
                print("âš ï¸ æ²¡æœ‰æ‰¾åˆ°ç°æœ‰æ–‡æ¡£")
                return
            
            print(f"ğŸ“„ æ‰¾åˆ° {len(documents)} ä¸ªç°æœ‰æ–‡æ¡£")
            
            # åˆ é™¤ç°æœ‰é›†åˆ
            self.delete_collection()
            
            # é‡æ–°åˆå§‹åŒ–ä½¿ç”¨æ–°æ¨¡å‹
            new_config = EmbeddingConfig(
                model_name=new_model_name,
                model_type="sentence_transformers",
                batch_size=self.embedding_config.batch_size,
                normalize_embeddings=self.embedding_config.normalize_embeddings,
                cache_enabled=False  # ç¦ç”¨ç¼“å­˜ä»¥å¼ºåˆ¶é‡æ–°ç”Ÿæˆ
            )
            
            self.embedding_config = new_config
            self.embedding_manager = EmbeddingManager(new_config)
            self.embedding_manager.initialize()
            
            # é‡æ–°åˆ›å»ºé›†åˆ
            self.collection = self.client.get_or_create_collection(
                name="enhanced_ai_papers_v2",
                metadata={"hnsw:space": "cosine"}
            )
            
            # é‡æ–°æ·»åŠ æ–‡æ¡£
            self.add_papers_with_metadata(documents, metadatas)
            
            print("âœ… åµŒå…¥é‡å»ºå®Œæˆ")
            
        except Exception as e:
            print(f"âŒ åµŒå…¥é‡å»ºå¤±è´¥: {e}")

# å‘åå…¼å®¹æ€§ - ä¸ºäº†ä¸ç ´åç°æœ‰ä»£ç 
class VectorStore(EnhancedVectorStore):
    """å‘åå…¼å®¹çš„å‘é‡å­˜å‚¨ç±»"""
    
    def __init__(self, persist_directory="vector_db"):
        # ä½¿ç”¨æ›´å¥½çš„é»˜è®¤æ¨¡å‹ï¼Œä½†ä¿æŒå‘åå…¼å®¹
        super().__init__(
            persist_directory=persist_directory,
            embedding_model="all-mpnet-base-v2"  # å‡çº§åˆ°æ›´å¥½çš„æ¨¡å‹
        )
        
        # æ˜¾ç¤ºå‡çº§æç¤º
        print("ğŸ”„ å·²è‡ªåŠ¨å‡çº§åˆ°å¢å¼ºç‰ˆåµŒå…¥æ¨¡å‹")
        print("   - ä» all-MiniLM-L6-v2 (384ç»´) å‡çº§åˆ° all-mpnet-base-v2 (768ç»´)")
        print("   - é¢„æœŸæ£€ç´¢è´¨é‡æ˜¾è‘—æå‡")
    
    def add_papers(self, papers: List[Dict]):
        """å…¼å®¹åŸæœ‰çš„æ·»åŠ è®ºæ–‡æ–¹æ³•"""
        print(f"ğŸ“ æ·»åŠ  {len(papers)} ç¯‡è®ºæ–‡åˆ°å¢å¼ºå‘é‡æ•°æ®åº“...")
        
        documents = []
        metadatas = []
        
        for paper in papers:
            doc_text = f"Title: {paper['title']}\n\nAbstract: {paper['abstract']}"
            documents.append(doc_text)
            
            metadata = {
                'title': paper['title'],
                'authors': ', '.join(paper['authors']) if isinstance(paper['authors'], list) else str(paper['authors']),
                'published': paper['published'],
                'paper_id': paper['id'],
                'chunk_id': f"{paper['id']}_main"
            }
            metadatas.append(metadata)
        
        # ä½¿ç”¨æ–°çš„æ–¹æ³•æ·»åŠ 
        self.add_papers_with_metadata(documents, metadatas)