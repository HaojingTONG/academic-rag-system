import chromadb
import numpy as np
from typing import List, Dict
from sentence_transformers import SentenceTransformer
import torch

class VectorStore:
    def __init__(self, persist_directory="vector_db", embedding_model=None):
        """
        åŸºç¡€å‘é‡å­˜å‚¨ - ç¡®ä¿åµŒå…¥æ¨¡å‹ä¸€è‡´æ€§
        
        Args:
            persist_directory: å­˜å‚¨ç›®å½•
            embedding_model: åµŒå…¥æ¨¡å‹åç§°ï¼Œå¦‚æœä¸æä¾›åˆ™ä½¿ç”¨ç³»ç»Ÿé»˜è®¤
        """
        from src.config.embedding_config import SystemEmbeddingConfig
        
        self.client = chromadb.PersistentClient(path=persist_directory)
        self.collection = self.client.get_or_create_collection(
            name="ai_papers_enhanced",  # æ–°çš„collectionåç§°
            metadata={"hnsw:space": "cosine"}
        )
        
        # ä½¿ç”¨ç³»ç»Ÿç»Ÿä¸€çš„åµŒå…¥æ¨¡å‹é…ç½®
        if embedding_model is None:
            embedding_model = SystemEmbeddingConfig.DEFAULT_MODEL_NAME
        
        # éªŒè¯åµŒå…¥æ¨¡å‹ä¸€è‡´æ€§
        if embedding_model != SystemEmbeddingConfig.DEFAULT_MODEL_NAME:
            print(f"âš ï¸ è­¦å‘Š: åµŒå…¥æ¨¡å‹ä¸ä¸€è‡´!")
            print(f"   ç³»ç»Ÿé»˜è®¤: {SystemEmbeddingConfig.DEFAULT_MODEL_NAME}")
            print(f"   å½“å‰ä½¿ç”¨: {embedding_model}")
            print("ğŸš¨ è¿™å¯èƒ½å¯¼è‡´å‘é‡ç©ºé—´ä¸ä¸€è‡´!")
        
        # åˆå§‹åŒ–åµŒå…¥æ¨¡å‹
        device = "mps" if torch.backends.mps.is_available() else "cpu"
        self.encoder = SentenceTransformer(embedding_model, device=device)
        self.encoder.model_name = embedding_model  # æ·»åŠ æ¨¡å‹åç§°å±æ€§ä¾¿äºéªŒè¯
        
        print(f"ğŸ”¥ åŸºç¡€å‘é‡å­˜å‚¨é…ç½®:")
        print(f"   - åµŒå…¥æ¨¡å‹: {embedding_model}")
        print(f"   - è®¡ç®—è®¾å¤‡: {device}")
        print(f"   - å‘é‡ç»´åº¦: {self.encoder.get_sentence_embedding_dimension()}")
    
    def add_papers(self, papers: List[Dict]):
        """æ·»åŠ è®ºæ–‡åˆ°å‘é‡æ•°æ®åº“ï¼ˆä¿ç•™åŸæœ‰æ–¹æ³•å…¼å®¹æ€§ï¼‰"""
        print(f"ğŸ“ æ·»åŠ {len(papers)}ç¯‡è®ºæ–‡åˆ°å‘é‡æ•°æ®åº“...")
        
        documents = []
        metadatas = []
        ids = []
        
        for paper in papers:
            doc_text = f"Title: {paper['title']}\n\nAbstract: {paper['abstract']}"
            documents.append(doc_text)
            
            metadata = {
                'title': paper['title'],
                'authors': ', '.join(paper['authors']) if isinstance(paper['authors'], list) else str(paper['authors']),
                'published': paper['published'],
                'paper_id': paper['id']
            }
            metadatas.append(metadata)
            ids.append(f"paper_{paper['id']}")
        
        # ç”ŸæˆåµŒå…¥
        print("ğŸ”¢ ç”Ÿæˆå‘é‡åµŒå…¥...")
        embeddings = self.encoder.encode(documents, show_progress_bar=True)
        
        # æ·»åŠ åˆ°æ•°æ®åº“
        self.collection.add(
            embeddings=embeddings.tolist(),
            documents=documents,
            metadatas=metadatas,
            ids=ids
        )
        
        print("âœ… è®ºæ–‡æ·»åŠ å®Œæˆ")
    
    def add_papers_with_metadata(self, documents: List[str], metadatas: List[Dict]):
        """æ·»åŠ å¸¦æœ‰å¢å¼ºå…ƒæ•°æ®çš„æ–‡æ¡£å—"""
        print(f"ğŸ“ æ·»åŠ {len(documents)}ä¸ªå¢å¼ºæ–‡æ¡£å—åˆ°å‘é‡æ•°æ®åº“...")
        
        # ç”Ÿæˆå”¯ä¸€ID
        ids = [f"enhanced_chunk_{i}_{metadata.get('chunk_id', 'unknown')}" 
               for i, metadata in enumerate(metadatas)]
        
        # ç”ŸæˆåµŒå…¥
        print("ğŸ”¢ ç”Ÿæˆå‘é‡åµŒå…¥...")
        embeddings = self.encoder.encode(documents, show_progress_bar=True)
        
        # æ‰¹é‡æ·»åŠ åˆ°æ•°æ®åº“
        batch_size = 100  # åˆ†æ‰¹å¤„ç†é¿å…å†…å­˜é—®é¢˜
        for i in range(0, len(documents), batch_size):
            end_idx = min(i + batch_size, len(documents))
            
            batch_embeddings = embeddings[i:end_idx]
            batch_documents = documents[i:end_idx]
            batch_metadatas = metadatas[i:end_idx]
            batch_ids = ids[i:end_idx]
            
            self.collection.add(
                embeddings=batch_embeddings.tolist(),
                documents=batch_documents,
                metadatas=batch_metadatas,
                ids=batch_ids
            )
            
            print(f"âœ… å·²å¤„ç† {end_idx}/{len(documents)} ä¸ªæ–‡æ¡£å—")
        
        print("âœ… å¢å¼ºæ–‡æ¡£å—æ·»åŠ å®Œæˆ")
    
    def search(self, query: str, top_k: int = 5, filter_metadata: Dict = None) -> Dict:
        """æœç´¢ç›¸å…³è®ºæ–‡ï¼ˆæ”¯æŒå…ƒæ•°æ®è¿‡æ»¤ï¼‰"""
        # æŸ¥è¯¢å‘é‡åŒ–
        query_embedding = self.encoder.encode([query])
        
        # æ„å»ºæœç´¢å‚æ•°
        search_params = {
            "query_embeddings": query_embedding.tolist(),
            "n_results": top_k,
            "include": ["documents", "metadatas", "distances"]
        }
        
        # æ·»åŠ è¿‡æ»¤æ¡ä»¶
        if filter_metadata:
            search_params["where"] = filter_metadata
        
        # æœç´¢
        results = self.collection.query(**search_params)
        
        return {
            'documents': results['documents'][0],
            'metadatas': results['metadatas'][0],
            'distances': results['distances'][0]
        }
    
    def search_by_section(self, query: str, section_type: str, top_k: int = 5) -> Dict:
        """æŒ‰ç« èŠ‚ç±»å‹æœç´¢"""
        filter_condition = {"section_type": section_type}
        return self.search(query, top_k, filter_condition)
    
    def search_with_formulas(self, query: str, top_k: int = 5) -> Dict:
        """æœç´¢åŒ…å«å…¬å¼çš„å†…å®¹"""
        filter_condition = {"has_formulas": True}
        return self.search(query, top_k, filter_condition)
    
    def search_with_code(self, query: str, top_k: int = 5) -> Dict:
        """æœç´¢åŒ…å«ä»£ç çš„å†…å®¹"""
        filter_condition = {"has_code": True}
        return self.search(query, top_k, filter_condition)
    
    def get_statistics(self) -> Dict:
        """è·å–å‘é‡æ•°æ®åº“ç»Ÿè®¡ä¿¡æ¯"""
        try:
            all_data = self.collection.get()
            total_docs = len(all_data['documents'])
            
            if total_docs == 0:
                return {"total_documents": 0, "statistics": "ç©ºæ•°æ®åº“"}
            
            # ç»Ÿè®¡å…ƒæ•°æ®ä¿¡æ¯
            metadatas = all_data['metadatas']
            
            # ç« èŠ‚ç±»å‹åˆ†å¸ƒ
            section_distribution = {}
            papers_distribution = {}
            content_features = {"has_formulas": 0, "has_code": 0, "has_citations": 0}
            
            for metadata in metadatas:
                # ç« èŠ‚åˆ†å¸ƒ
                section_type = metadata.get('section_type', 'unknown')
                section_distribution[section_type] = section_distribution.get(section_type, 0) + 1
                
                # è®ºæ–‡åˆ†å¸ƒ
                paper_id = metadata.get('paper_id', 'unknown')
                papers_distribution[paper_id] = papers_distribution.get(paper_id, 0) + 1
                
                # å†…å®¹ç‰¹å¾
                for feature in content_features:
                    if metadata.get(feature, False):
                        content_features[feature] += 1
            
            return {
                "total_documents": total_docs,
                "unique_papers": len(papers_distribution),
                "section_distribution": section_distribution,
                "content_features": content_features,
                "avg_chunks_per_paper": total_docs / len(papers_distribution) if papers_distribution else 0
            }
            
        except Exception as e:
            return {"error": f"ç»Ÿè®¡ä¿¡æ¯è·å–å¤±è´¥: {e}"}
    
    def clear_database(self):
        """æ¸…ç©ºæ•°æ®åº“"""
        try:
            self.client.delete_collection("ai_papers_enhanced")
            self.collection = self.client.get_or_create_collection(
                name="ai_papers_enhanced",
                metadata={"hnsw:space": "cosine"}
            )
            print("âœ… æ•°æ®åº“å·²æ¸…ç©º")
        except Exception as e:
            print(f"âŒ æ¸…ç©ºæ•°æ®åº“å¤±è´¥: {e}")

if __name__ == "__main__":
    # æµ‹è¯•å‘é‡å­˜å‚¨ç³»ç»Ÿ
    vs = VectorStore()
    print("å‘é‡å­˜å‚¨ç³»ç»Ÿæµ‹è¯•å®Œæˆ")
    
    # æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
    stats = vs.get_statistics()
    print(f"æ•°æ®åº“ç»Ÿè®¡: {stats}")