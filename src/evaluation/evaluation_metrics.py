# src/evaluation/evaluation_metrics.py
"""
RAGç³»ç»Ÿè¯„ä¼°æŒ‡æ ‡å®ç°
åŒ…æ‹¬ä¸Šä¸‹æ–‡ç›¸å…³æ€§ã€ç­”æ¡ˆå¿ å®åº¦ã€ç­”æ¡ˆç›¸å…³æ€§ç­‰æ ¸å¿ƒæŒ‡æ ‡
"""

import re
import json
import numpy as np
from typing import Dict, List, Tuple, Optional, Any
from dataclasses import dataclass
from sentence_transformers import SentenceTransformer, util
import requests
import time

@dataclass 
class EvaluationResult:
    """è¯„ä¼°ç»“æœæ•°æ®ç»“æ„"""
    context_relevance: float      # ä¸Šä¸‹æ–‡ç›¸å…³æ€§ (0-1)
    faithfulness: float          # ç­”æ¡ˆå¿ å®åº¦ (0-1)
    answer_relevance: float      # ç­”æ¡ˆç›¸å…³æ€§ (0-1)
    context_precision: float     # ä¸Šä¸‹æ–‡ç²¾ç¡®åº¦ (0-1)
    context_recall: float        # ä¸Šä¸‹æ–‡å¬å›ç‡ (0-1)
    overall_score: float         # ç»¼åˆè¯„åˆ† (0-1)
    detailed_scores: Dict[str, Any]  # è¯¦ç»†åˆ†æ•°
    
    def to_dict(self) -> Dict:
        """è½¬æ¢ä¸ºå­—å…¸æ ¼å¼"""
        return {
            'context_relevance': self.context_relevance,
            'faithfulness': self.faithfulness,
            'answer_relevance': self.answer_relevance,
            'context_precision': self.context_precision,
            'context_recall': self.context_recall,
            'overall_score': self.overall_score,
            'detailed_scores': self.detailed_scores
        }

class EvaluationMetrics:
    """RAGè¯„ä¼°æŒ‡æ ‡è®¡ç®—å™¨"""
    
    def __init__(self, embedding_model: str = "all-MiniLM-L6-v2"):
        """
        åˆå§‹åŒ–è¯„ä¼°æŒ‡æ ‡è®¡ç®—å™¨
        
        Args:
            embedding_model: ç”¨äºè®¡ç®—è¯­ä¹‰ç›¸ä¼¼åº¦çš„æ¨¡å‹
        """
        self.embedding_model = SentenceTransformer(embedding_model)
        print(f"âœ… åŠ è½½è¯„ä¼°ç”¨åµŒå…¥æ¨¡å‹: {embedding_model}")
        
    def evaluate_single_case(self, 
                           question: str,
                           retrieved_contexts: List[str],
                           generated_answer: str,
                           ground_truth: Optional[str] = None,
                           relevant_contexts: Optional[List[str]] = None) -> EvaluationResult:
        """
        è¯„ä¼°å•ä¸ªé—®ç­”æ¡ˆä¾‹
        
        Args:
            question: ç”¨æˆ·é—®é¢˜
            retrieved_contexts: æ£€ç´¢åˆ°çš„ä¸Šä¸‹æ–‡åˆ—è¡¨
            generated_answer: ç”Ÿæˆçš„ç­”æ¡ˆ
            ground_truth: æ ‡å‡†ç­”æ¡ˆ (å¯é€‰)
            relevant_contexts: ç›¸å…³ä¸Šä¸‹æ–‡åˆ—è¡¨ (ç”¨äºè®¡ç®—å¬å›ç‡, å¯é€‰)
            
        Returns:
            EvaluationResult: è¯„ä¼°ç»“æœ
        """
        print(f"ğŸ“Š è¯„ä¼°é—®é¢˜: {question[:50]}...")
        
        # 1. è®¡ç®—ä¸Šä¸‹æ–‡ç›¸å…³æ€§
        context_relevance = self._calculate_context_relevance(question, retrieved_contexts)
        
        # 2. è®¡ç®—ç­”æ¡ˆå¿ å®åº¦  
        faithfulness = self._calculate_faithfulness(generated_answer, retrieved_contexts)
        
        # 3. è®¡ç®—ç­”æ¡ˆç›¸å…³æ€§
        answer_relevance = self._calculate_answer_relevance(question, generated_answer)
        
        # 4. è®¡ç®—ä¸Šä¸‹æ–‡ç²¾ç¡®åº¦
        context_precision = self._calculate_context_precision(question, retrieved_contexts)
        
        # 5. è®¡ç®—ä¸Šä¸‹æ–‡å¬å›ç‡
        context_recall = self._calculate_context_recall(retrieved_contexts, relevant_contexts) if relevant_contexts else 0.0
        
        # 6. è®¡ç®—ç»¼åˆè¯„åˆ†
        overall_score = self._calculate_overall_score(
            context_relevance, faithfulness, answer_relevance, context_precision, context_recall
        )
        
        # è¯¦ç»†åˆ†æ•°
        detailed_scores = {
            'semantic_similarity': self._calculate_semantic_similarity(question, generated_answer),
            'context_coverage': self._calculate_context_coverage(generated_answer, retrieved_contexts),
            'answer_length': len(generated_answer.split()),
            'context_count': len(retrieved_contexts),
            'avg_context_length': np.mean([len(ctx.split()) for ctx in retrieved_contexts]) if retrieved_contexts else 0
        }
        
        result = EvaluationResult(
            context_relevance=context_relevance,
            faithfulness=faithfulness,
            answer_relevance=answer_relevance,
            context_precision=context_precision,
            context_recall=context_recall,
            overall_score=overall_score,
            detailed_scores=detailed_scores
        )
        
        print(f"   âœ… è¯„ä¼°å®Œæˆ - ç»¼åˆè¯„åˆ†: {overall_score:.3f}")
        return result
    
    def _calculate_context_relevance(self, question: str, contexts: List[str]) -> float:
        """è®¡ç®—ä¸Šä¸‹æ–‡ç›¸å…³æ€§ - æ£€ç´¢åˆ°çš„ä¸Šä¸‹æ–‡ä¸é—®é¢˜çš„ç›¸å…³ç¨‹åº¦"""
        if not contexts:
            return 0.0
            
        question_embedding = self.embedding_model.encode([question])
        context_embeddings = self.embedding_model.encode(contexts)
        
        similarities = util.cos_sim(question_embedding, context_embeddings)[0]
        
        # è®¡ç®—å¹³å‡ç›¸å…³æ€§ï¼Œä½†ç»™é«˜ç›¸å…³æ€§æ›´å¤šæƒé‡
        similarities = similarities.numpy()
        weighted_relevance = np.mean(similarities) * 0.7 + np.max(similarities) * 0.3
        
        return float(np.clip(weighted_relevance, 0, 1))
    
    def _calculate_faithfulness(self, answer: str, contexts: List[str]) -> float:
        """è®¡ç®—ç­”æ¡ˆå¿ å®åº¦ - ç­”æ¡ˆæ˜¯å¦åŸºäºæä¾›çš„ä¸Šä¸‹æ–‡"""
        if not contexts or not answer:
            return 0.0
            
        # å°†æ‰€æœ‰ä¸Šä¸‹æ–‡åˆå¹¶
        combined_context = " ".join(contexts)
        
        # è®¡ç®—è¯­ä¹‰ç›¸ä¼¼åº¦
        answer_embedding = self.embedding_model.encode([answer])
        context_embedding = self.embedding_model.encode([combined_context])
        
        semantic_similarity = util.cos_sim(answer_embedding, context_embedding)[0][0]
        
        # è®¡ç®—è¯æ±‡é‡å åº¦
        answer_words = set(answer.lower().split())
        context_words = set(combined_context.lower().split())
        
        if len(answer_words) == 0:
            lexical_overlap = 0.0
        else:
            lexical_overlap = len(answer_words.intersection(context_words)) / len(answer_words)
        
        # ç»¼åˆè¯­ä¹‰å’Œè¯æ±‡ç›¸ä¼¼åº¦
        faithfulness = float(semantic_similarity * 0.7 + lexical_overlap * 0.3)
        
        return np.clip(faithfulness, 0, 1)
    
    def _calculate_answer_relevance(self, question: str, answer: str) -> float:
        """è®¡ç®—ç­”æ¡ˆç›¸å…³æ€§ - ç­”æ¡ˆæ˜¯å¦ç›´æ¥å›ç­”äº†é—®é¢˜"""
        if not question or not answer:
            return 0.0
            
        question_embedding = self.embedding_model.encode([question])
        answer_embedding = self.embedding_model.encode([answer])
        
        similarity = util.cos_sim(question_embedding, answer_embedding)[0][0]
        
        return float(np.clip(similarity, 0, 1))
    
    def _calculate_context_precision(self, question: str, contexts: List[str]) -> float:
        """è®¡ç®—ä¸Šä¸‹æ–‡ç²¾ç¡®åº¦ - æ£€ç´¢åˆ°çš„ä¸Šä¸‹æ–‡ä¸­æœ‰å¤šå°‘æ˜¯ç›¸å…³çš„"""
        if not contexts:
            return 0.0
            
        question_embedding = self.embedding_model.encode([question])
        context_embeddings = self.embedding_model.encode(contexts)
        
        similarities = util.cos_sim(question_embedding, context_embeddings)[0]
        
        # è®¾å®šç›¸å…³æ€§é˜ˆå€¼
        relevance_threshold = 0.5
        relevant_count = sum(1 for sim in similarities if sim > relevance_threshold)
        
        precision = relevant_count / len(contexts)
        return float(precision)
    
    def _calculate_context_recall(self, retrieved_contexts: List[str], 
                                relevant_contexts: Optional[List[str]]) -> float:
        """è®¡ç®—ä¸Šä¸‹æ–‡å¬å›ç‡ - ç›¸å…³æ–‡æ¡£ä¸­æœ‰å¤šå°‘è¢«æ£€ç´¢åˆ°"""
        if not relevant_contexts or not retrieved_contexts:
            return 0.0
            
        retrieved_embeddings = self.embedding_model.encode(retrieved_contexts)
        relevant_embeddings = self.embedding_model.encode(relevant_contexts)
        
        # è®¡ç®—æ£€ç´¢åˆ°çš„ä¸Šä¸‹æ–‡ä¸ç›¸å…³ä¸Šä¸‹æ–‡çš„ç›¸ä¼¼åº¦
        similarities = util.cos_sim(retrieved_embeddings, relevant_embeddings)
        
        # å¯¹æ¯ä¸ªç›¸å…³æ–‡æ¡£ï¼Œæ£€æŸ¥æ˜¯å¦æœ‰è¶³å¤Ÿç›¸ä¼¼çš„æ£€ç´¢ç»“æœ
        similarity_threshold = 0.7
        recalled_count = 0
        
        for i in range(len(relevant_contexts)):
            max_similarity = np.max(similarities[:, i])
            if max_similarity > similarity_threshold:
                recalled_count += 1
                
        recall = recalled_count / len(relevant_contexts)
        return float(recall)
    
    def _calculate_semantic_similarity(self, text1: str, text2: str) -> float:
        """è®¡ç®—ä¸¤ä¸ªæ–‡æœ¬çš„è¯­ä¹‰ç›¸ä¼¼åº¦"""
        if not text1 or not text2:
            return 0.0
            
        embeddings = self.embedding_model.encode([text1, text2])
        similarity = util.cos_sim(embeddings[0:1], embeddings[1:2])[0][0]
        
        return float(similarity)
    
    def _calculate_context_coverage(self, answer: str, contexts: List[str]) -> float:
        """è®¡ç®—ç­”æ¡ˆå¯¹ä¸Šä¸‹æ–‡çš„è¦†ç›–ç¨‹åº¦"""
        if not contexts or not answer:
            return 0.0
            
        answer_words = set(answer.lower().split())
        context_words = set()
        
        for context in contexts:
            context_words.update(context.lower().split())
            
        if len(context_words) == 0:
            return 0.0
            
        coverage = len(answer_words.intersection(context_words)) / len(context_words)
        return float(np.clip(coverage, 0, 1))
    
    def _calculate_overall_score(self, context_relevance: float, faithfulness: float,
                               answer_relevance: float, context_precision: float,
                               context_recall: float) -> float:
        """è®¡ç®—ç»¼åˆè¯„åˆ†"""
        # æƒé‡è®¾ç½®
        weights = {
            'context_relevance': 0.25,
            'faithfulness': 0.30,
            'answer_relevance': 0.25, 
            'context_precision': 0.15,
            'context_recall': 0.05  # å¬å›ç‡æƒé‡è¾ƒä½ï¼Œå› ä¸ºé€šå¸¸æ²¡æœ‰å®Œæ•´çš„ç›¸å…³æ–‡æ¡£é›†
        }
        
        overall_score = (
            context_relevance * weights['context_relevance'] +
            faithfulness * weights['faithfulness'] +
            answer_relevance * weights['answer_relevance'] +
            context_precision * weights['context_precision'] +
            context_recall * weights['context_recall']
        )
        
        return float(np.clip(overall_score, 0, 1))

class LLMBasedEvaluator:
    """åŸºäºLLMçš„è¯„ä¼°å™¨ - ä½¿ç”¨å¤§è¯­è¨€æ¨¡å‹è¿›è¡Œæ›´æ·±å…¥çš„è¯„ä¼°"""
    
    def __init__(self, llm_base_url: str = "http://localhost:11434", model: str = "llama3.1:8b"):
        """
        åˆå§‹åŒ–LLMè¯„ä¼°å™¨
        
        Args:
            llm_base_url: LLMæœåŠ¡çš„åŸºç¡€URL
            model: ä½¿ç”¨çš„æ¨¡å‹åç§°
        """
        self.base_url = llm_base_url
        self.model = model
        print(f"âœ… åˆå§‹åŒ–LLMè¯„ä¼°å™¨: {model}")
        
    def evaluate_with_llm(self, question: str, context: str, answer: str) -> Dict[str, Any]:
        """ä½¿ç”¨LLMè¯„ä¼°é—®ç­”è´¨é‡"""
        
        evaluation_prompt = f"""
è¯·ä½œä¸ºä¸€ä¸ªä¸“ä¸šçš„RAGç³»ç»Ÿè¯„ä¼°ä¸“å®¶ï¼Œä»ä»¥ä¸‹å‡ ä¸ªç»´åº¦è¯„ä¼°è¿™ä¸ªé—®ç­”æ¡ˆä¾‹çš„è´¨é‡ï¼š

é—®é¢˜: {question}

ä¸Šä¸‹æ–‡: {context}

ç”Ÿæˆçš„ç­”æ¡ˆ: {answer}

è¯·ä»ä»¥ä¸‹ç»´åº¦è¿›è¡Œè¯„ä¼°ï¼ˆ1-5åˆ†ï¼Œ5åˆ†æœ€å¥½ï¼‰ï¼š

1. ä¸Šä¸‹æ–‡ç›¸å…³æ€§: æä¾›çš„ä¸Šä¸‹æ–‡ä¸é—®é¢˜çš„ç›¸å…³ç¨‹åº¦
2. ç­”æ¡ˆå¿ å®åº¦: ç­”æ¡ˆæ˜¯å¦å®Œå…¨åŸºäºæä¾›çš„ä¸Šä¸‹æ–‡ï¼Œæ²¡æœ‰è™šæ„ä¿¡æ¯  
3. ç­”æ¡ˆç›¸å…³æ€§: ç­”æ¡ˆæ˜¯å¦ç›´æ¥å›ç­”äº†ç”¨æˆ·çš„é—®é¢˜
4. ç­”æ¡ˆå®Œæ•´æ€§: ç­”æ¡ˆæ˜¯å¦å®Œæ•´ï¼Œæœ‰æ²¡æœ‰é—æ¼é‡è¦ä¿¡æ¯
5. ç­”æ¡ˆæ¸…æ™°åº¦: ç­”æ¡ˆæ˜¯å¦è¡¨è¾¾æ¸…æ™°ï¼Œæ˜“äºç†è§£

è¯·ä»¥JSONæ ¼å¼è¿”å›è¯„ä¼°ç»“æœï¼š
{{
  "context_relevance": <åˆ†æ•°>,
  "faithfulness": <åˆ†æ•°>,
  "answer_relevance": <åˆ†æ•°>, 
  "completeness": <åˆ†æ•°>,
  "clarity": <åˆ†æ•°>,
  "reasoning": "<è¯¦ç»†çš„è¯„ä¼°ç†ç”±>"
}}
"""
        
        try:
            response = requests.post(
                f"{self.base_url}/api/generate",
                json={
                    "model": self.model,
                    "prompt": evaluation_prompt,
                    "stream": False,
                    "options": {
                        "temperature": 0.1,
                        "top_p": 0.9
                    }
                },
                timeout=60
            )
            
            if response.status_code == 200:
                result = response.json()
                answer_text = result.get('response', '')
                
                # å°è¯•è§£æJSONç»“æœ
                try:
                    # æå–JSONéƒ¨åˆ†
                    json_start = answer_text.find('{')
                    json_end = answer_text.rfind('}') + 1
                    if json_start != -1 and json_end != 0:
                        json_str = answer_text[json_start:json_end]
                        evaluation_result = json.loads(json_str)
                        
                        # æ ‡å‡†åŒ–è¯„åˆ† (1-5 -> 0-1)
                        for key in ['context_relevance', 'faithfulness', 'answer_relevance', 'completeness', 'clarity']:
                            if key in evaluation_result:
                                evaluation_result[key] = (evaluation_result[key] - 1) / 4
                                
                        return evaluation_result
                except json.JSONDecodeError:
                    pass
                    
            return {"error": f"LLMè¯„ä¼°å¤±è´¥: {response.status_code}"}
            
        except Exception as e:
            return {"error": f"LLMè¯„ä¼°å¼‚å¸¸: {str(e)}"}

# ä½¿ç”¨ç¤ºä¾‹å’Œæµ‹è¯•å‡½æ•°
def test_evaluation_metrics():
    """æµ‹è¯•è¯„ä¼°æŒ‡æ ‡"""
    print("ğŸ§ª æµ‹è¯•è¯„ä¼°æŒ‡æ ‡...")
    
    evaluator = EvaluationMetrics()
    
    # æµ‹è¯•ç”¨ä¾‹
    question = "What is attention mechanism in neural networks?"
    contexts = [
        "Attention mechanism allows neural networks to focus on relevant parts of input sequences.",
        "The attention mechanism was introduced in the Transformer architecture.",
        "Attention computes weighted averages of input representations."
    ]
    answer = "Attention mechanism is a technique that allows neural networks to selectively focus on relevant parts of the input sequence when making predictions."
    
    result = evaluator.evaluate_single_case(question, contexts, answer)
    
    print(f"ğŸ“Š è¯„ä¼°ç»“æœ:")
    print(f"   ä¸Šä¸‹æ–‡ç›¸å…³æ€§: {result.context_relevance:.3f}")
    print(f"   ç­”æ¡ˆå¿ å®åº¦: {result.faithfulness:.3f}") 
    print(f"   ç­”æ¡ˆç›¸å…³æ€§: {result.answer_relevance:.3f}")
    print(f"   ç»¼åˆè¯„åˆ†: {result.overall_score:.3f}")
    
    return result

if __name__ == "__main__":
    test_evaluation_metrics()