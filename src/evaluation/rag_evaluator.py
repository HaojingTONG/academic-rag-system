# src/evaluation/rag_evaluator.py
"""
RAGç³»ç»Ÿå®Œæ•´è¯„ä¼°å™¨
æ•´åˆå„ç§è¯„ä¼°æŒ‡æ ‡å’Œæ•°æ®é›†ï¼Œæä¾›å®Œæ•´çš„RAGç³»ç»Ÿè¯„ä¼°åŠŸèƒ½
"""

import json
import time
from typing import Dict, List, Any, Optional, Tuple
from pathlib import Path
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from datetime import datetime
import numpy as np

from .evaluation_metrics import EvaluationMetrics, EvaluationResult, LLMBasedEvaluator
from .benchmark_datasets import BenchmarkDatasets, EvaluationCase

class RAGEvaluator:
    """RAGç³»ç»Ÿè¯„ä¼°å™¨ä¸»ç±»"""
    
    def __init__(self, 
                 embedding_model: str = "all-MiniLM-L6-v2",
                 llm_base_url: str = "http://localhost:11434",
                 llm_model: str = "llama3.1:8b",
                 output_dir: str = "data/evaluation/results"):
        """
        åˆå§‹åŒ–RAGè¯„ä¼°å™¨
        
        Args:
            embedding_model: ç”¨äºè¯­ä¹‰ç›¸ä¼¼åº¦è®¡ç®—çš„åµŒå…¥æ¨¡å‹
            llm_base_url: LLMæœåŠ¡URL
            llm_model: LLMæ¨¡å‹åç§°
            output_dir: è¯„ä¼°ç»“æœè¾“å‡ºç›®å½•
        """
        self.metrics = EvaluationMetrics(embedding_model)
        self.llm_evaluator = LLMBasedEvaluator(llm_base_url, llm_model)
        self.dataset_manager = BenchmarkDatasets()
        
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(exist_ok=True, parents=True)
        
        print(f"âœ… RAGè¯„ä¼°å™¨åˆå§‹åŒ–å®Œæˆ")
        print(f"   åµŒå…¥æ¨¡å‹: {embedding_model}")
        print(f"   LLMæ¨¡å‹: {llm_model}")
        print(f"   è¾“å‡ºç›®å½•: {output_dir}")
    
    def evaluate_rag_system(self, 
                           rag_system,
                           evaluation_dataset: List[EvaluationCase],
                           use_llm_evaluation: bool = True,
                           save_results: bool = True) -> Dict[str, Any]:
        """
        å®Œæ•´è¯„ä¼°RAGç³»ç»Ÿ
        
        Args:
            rag_system: RAGç³»ç»Ÿå®ä¾‹ï¼Œéœ€è¦æœ‰queryæ–¹æ³•
            evaluation_dataset: è¯„ä¼°æ•°æ®é›†
            use_llm_evaluation: æ˜¯å¦ä½¿ç”¨LLMè¿›è¡Œé¢å¤–è¯„ä¼°
            save_results: æ˜¯å¦ä¿å­˜è¯„ä¼°ç»“æœ
            
        Returns:
            Dict: è¯„ä¼°ç»“æœæŠ¥å‘Š
        """
        print(f"ğŸš€ å¼€å§‹è¯„ä¼°RAGç³»ç»Ÿ ({len(evaluation_dataset)} ä¸ªæµ‹è¯•æ¡ˆä¾‹)...")
        print("=" * 80)
        
        evaluation_results = []
        start_time = time.time()
        
        for i, case in enumerate(evaluation_dataset):
            print(f"\nğŸ“‹ æ¡ˆä¾‹ [{i+1}/{len(evaluation_dataset)}]: {case.question[:60]}...")
            print(f"   éš¾åº¦: {case.difficulty} | ç±»åˆ«: {case.category}")
            
            # ä½¿ç”¨RAGç³»ç»Ÿç”Ÿæˆç­”æ¡ˆ
            try:
                rag_response = rag_system.query(case.question)
                
                # æå–RAGç³»ç»Ÿè¿”å›çš„ä¿¡æ¯
                if isinstance(rag_response, dict):
                    generated_answer = rag_response.get('answer', '')
                    retrieved_contexts = rag_response.get('contexts', [])
                elif isinstance(rag_response, str):
                    generated_answer = rag_response
                    retrieved_contexts = []  # æ— æ³•è·å–æ£€ç´¢ä¸Šä¸‹æ–‡
                else:
                    print(f"   âš ï¸ æ— æ³•è§£æRAGç³»ç»Ÿå“åº”: {type(rag_response)}")
                    continue
                    
            except Exception as e:
                print(f"   âŒ RAGç³»ç»ŸæŸ¥è¯¢å¤±è´¥: {e}")
                continue
            
            # æ ¸å¿ƒæŒ‡æ ‡è¯„ä¼°
            metric_result = self.metrics.evaluate_single_case(
                question=case.question,
                retrieved_contexts=retrieved_contexts,
                generated_answer=generated_answer,
                ground_truth=case.ground_truth_answer,
                relevant_contexts=case.relevant_contexts
            )
            
            # LLMè¯„ä¼°ï¼ˆå¯é€‰ï¼‰
            llm_result = {}
            if use_llm_evaluation and retrieved_contexts:
                combined_context = "\n".join(retrieved_contexts)
                llm_result = self.llm_evaluator.evaluate_with_llm(
                    case.question, combined_context, generated_answer
                )
            
            # æ•´åˆç»“æœ
            case_result = {
                'case_id': case.id,
                'question': case.question,
                'difficulty': case.difficulty,
                'category': case.category,
                'generated_answer': generated_answer,
                'ground_truth_answer': case.ground_truth_answer,
                'retrieved_contexts': retrieved_contexts,
                'metric_scores': metric_result.to_dict(),
                'llm_scores': llm_result,
                'metadata': case.metadata
            }
            
            evaluation_results.append(case_result)
            
            print(f"   âœ… è¯„ä¼°å®Œæˆ - ç»¼åˆè¯„åˆ†: {metric_result.overall_score:.3f}")
            
            # é¿å…è¯·æ±‚è¿‡äºé¢‘ç¹
            if use_llm_evaluation:
                time.sleep(0.5)
        
        # è®¡ç®—æ€»ä½“ç»Ÿè®¡
        evaluation_time = time.time() - start_time
        summary_report = self._generate_summary_report(evaluation_results, evaluation_time)
        
        # ä¿å­˜ç»“æœ
        if save_results:
            self._save_evaluation_results(evaluation_results, summary_report)
        
        print("\n" + "=" * 80)
        print("ğŸ“Š è¯„ä¼°å®Œæˆ!")
        self._print_summary_report(summary_report)
        
        return {
            'summary': summary_report,
            'detailed_results': evaluation_results
        }
    
    def _generate_summary_report(self, results: List[Dict], evaluation_time: float) -> Dict[str, Any]:
        """ç”Ÿæˆè¯„ä¼°æ‘˜è¦æŠ¥å‘Š"""
        if not results:
            return {}
            
        # æå–æ‰€æœ‰æŒ‡æ ‡åˆ†æ•°
        all_scores = []
        for result in results:
            if 'metric_scores' in result:
                all_scores.append(result['metric_scores'])
        
        if not all_scores:
            return {}
        
        # è®¡ç®—å¹³å‡åˆ†æ•°
        avg_scores = {}
        score_keys = ['context_relevance', 'faithfulness', 'answer_relevance', 
                     'context_precision', 'context_recall', 'overall_score']
        
        for key in score_keys:
            scores = [score.get(key, 0) for score in all_scores if key in score]
            if scores:
                avg_scores[f'avg_{key}'] = np.mean(scores)
                avg_scores[f'std_{key}'] = np.std(scores)
                avg_scores[f'min_{key}'] = np.min(scores)
                avg_scores[f'max_{key}'] = np.max(scores)
        
        # æŒ‰éš¾åº¦å’Œç±»åˆ«åˆ†ç»„ç»Ÿè®¡
        difficulty_stats = self._calculate_group_stats(results, 'difficulty')
        category_stats = self._calculate_group_stats(results, 'category')
        
        # ç”ŸæˆæŠ¥å‘Š
        summary = {
            'evaluation_info': {
                'total_cases': len(results),
                'evaluation_time': evaluation_time,
                'timestamp': datetime.now().isoformat()
            },
            'overall_performance': avg_scores,
            'difficulty_breakdown': difficulty_stats,
            'category_breakdown': category_stats,
            'top_performing_cases': self._get_top_cases(results, 'best'),
            'worst_performing_cases': self._get_top_cases(results, 'worst')
        }
        
        return summary
    
    def _calculate_group_stats(self, results: List[Dict], group_key: str) -> Dict[str, Any]:
        """è®¡ç®—åˆ†ç»„ç»Ÿè®¡ä¿¡æ¯"""
        groups = {}
        
        for result in results:
            group_value = result.get(group_key, 'unknown')
            if group_value not in groups:
                groups[group_value] = []
            
            if 'metric_scores' in result:
                overall_score = result['metric_scores'].get('overall_score', 0)
                groups[group_value].append(overall_score)
        
        group_stats = {}
        for group, scores in groups.items():
            if scores:
                group_stats[group] = {
                    'count': len(scores),
                    'avg_score': np.mean(scores),
                    'std_score': np.std(scores),
                    'min_score': np.min(scores),
                    'max_score': np.max(scores)
                }
        
        return group_stats
    
    def _get_top_cases(self, results: List[Dict], mode: str = 'best', n: int = 3) -> List[Dict]:
        """è·å–è¡¨ç°æœ€å¥½/æœ€å·®çš„æ¡ˆä¾‹"""
        scored_cases = []
        
        for result in results:
            if 'metric_scores' in result:
                overall_score = result['metric_scores'].get('overall_score', 0)
                scored_cases.append({
                    'case_id': result['case_id'],
                    'question': result['question'][:100] + '...' if len(result['question']) > 100 else result['question'],
                    'overall_score': overall_score,
                    'difficulty': result.get('difficulty', 'unknown'),
                    'category': result.get('category', 'unknown')
                })
        
        # æ’åº
        reverse = (mode == 'best')
        scored_cases.sort(key=lambda x: x['overall_score'], reverse=reverse)
        
        return scored_cases[:n]
    
    def _save_evaluation_results(self, results: List[Dict], summary: Dict[str, Any]):
        """ä¿å­˜è¯„ä¼°ç»“æœ"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # ä¿å­˜è¯¦ç»†ç»“æœ
        detailed_file = self.output_dir / f"evaluation_detailed_{timestamp}.json"
        with open(detailed_file, 'w', encoding='utf-8') as f:
            json.dump(results, f, ensure_ascii=False, indent=2)
        
        # ä¿å­˜æ‘˜è¦æŠ¥å‘Š
        summary_file = self.output_dir / f"evaluation_summary_{timestamp}.json"
        with open(summary_file, 'w', encoding='utf-8') as f:
            json.dump(summary, f, ensure_ascii=False, indent=2)
        
        # ç”ŸæˆCSVæŠ¥å‘Š
        self._generate_csv_report(results, timestamp)
        
        # ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨
        self._generate_visualizations(results, summary, timestamp)
        
        print(f"ğŸ’¾ è¯„ä¼°ç»“æœå·²ä¿å­˜:")
        print(f"   è¯¦ç»†ç»“æœ: {detailed_file}")
        print(f"   æ‘˜è¦æŠ¥å‘Š: {summary_file}")
    
    def _generate_csv_report(self, results: List[Dict], timestamp: str):
        """ç”ŸæˆCSVæ ¼å¼çš„è¯„ä¼°æŠ¥å‘Š"""
        csv_data = []
        
        for result in results:
            row = {
                'case_id': result['case_id'],
                'difficulty': result.get('difficulty', ''),
                'category': result.get('category', ''),
                'question_length': len(result['question']),
                'answer_length': len(result.get('generated_answer', '')),
                'context_count': len(result.get('retrieved_contexts', []))
            }
            
            # æ·»åŠ è¯„ä¼°æŒ‡æ ‡
            if 'metric_scores' in result:
                metrics = result['metric_scores']
                row.update({
                    'context_relevance': metrics.get('context_relevance', 0),
                    'faithfulness': metrics.get('faithfulness', 0),
                    'answer_relevance': metrics.get('answer_relevance', 0),
                    'context_precision': metrics.get('context_precision', 0),
                    'context_recall': metrics.get('context_recall', 0),
                    'overall_score': metrics.get('overall_score', 0)
                })
            
            csv_data.append(row)
        
        df = pd.DataFrame(csv_data)
        csv_file = self.output_dir / f"evaluation_report_{timestamp}.csv"
        df.to_csv(csv_file, index=False, encoding='utf-8')
        
        print(f"   CSVæŠ¥å‘Š: {csv_file}")
    
    def _generate_visualizations(self, results: List[Dict], summary: Dict[str, Any], timestamp: str):
        """ç”Ÿæˆè¯„ä¼°ç»“æœå¯è§†åŒ–å›¾è¡¨"""
        try:
            # å‡†å¤‡æ•°æ®
            scores_data = []
            for result in results:
                if 'metric_scores' in result:
                    metrics = result['metric_scores']
                    scores_data.append({
                        'difficulty': result.get('difficulty', 'unknown'),
                        'category': result.get('category', 'unknown'),
                        'context_relevance': metrics.get('context_relevance', 0),
                        'faithfulness': metrics.get('faithfulness', 0),
                        'answer_relevance': metrics.get('answer_relevance', 0),
                        'overall_score': metrics.get('overall_score', 0)
                    })
            
            if not scores_data:
                return
                
            df = pd.DataFrame(scores_data)
            
            # åˆ›å»ºå›¾è¡¨
            fig, axes = plt.subplots(2, 2, figsize=(15, 12))
            fig.suptitle('RAG System Evaluation Results', fontsize=16)
            
            # 1. æ•´ä½“åˆ†æ•°åˆ†å¸ƒ
            axes[0, 0].hist(df['overall_score'], bins=20, alpha=0.7, color='skyblue')
            axes[0, 0].set_title('Overall Score Distribution')
            axes[0, 0].set_xlabel('Overall Score')
            axes[0, 0].set_ylabel('Frequency')
            
            # 2. æŒ‰éš¾åº¦åˆ†ç»„çš„å¹³å‡åˆ†æ•°
            if 'difficulty' in df.columns:
                difficulty_scores = df.groupby('difficulty')['overall_score'].mean()
                axes[0, 1].bar(difficulty_scores.index, difficulty_scores.values, color='lightcoral')
                axes[0, 1].set_title('Average Score by Difficulty')
                axes[0, 1].set_xlabel('Difficulty')
                axes[0, 1].set_ylabel('Average Score')
            
            # 3. å„æŒ‡æ ‡å¯¹æ¯”
            metric_cols = ['context_relevance', 'faithfulness', 'answer_relevance']
            metric_means = [df[col].mean() for col in metric_cols]
            axes[1, 0].bar(metric_cols, metric_means, color='lightgreen')
            axes[1, 0].set_title('Average Scores by Metric')
            axes[1, 0].set_ylabel('Average Score')
            axes[1, 0].tick_params(axis='x', rotation=45)
            
            # 4. å„æŒ‡æ ‡çƒ­åŠ›å›¾
            if len(df) > 1:
                correlation_matrix = df[metric_cols + ['overall_score']].corr()
                sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0, ax=axes[1, 1])
                axes[1, 1].set_title('Metric Correlations')
            
            plt.tight_layout()
            
            # ä¿å­˜å›¾è¡¨
            chart_file = self.output_dir / f"evaluation_charts_{timestamp}.png"
            plt.savefig(chart_file, dpi=300, bbox_inches='tight')
            plt.close()
            
            print(f"   å¯è§†åŒ–å›¾è¡¨: {chart_file}")
            
        except Exception as e:
            print(f"   âš ï¸ å›¾è¡¨ç”Ÿæˆå¤±è´¥: {e}")
    
    def _print_summary_report(self, summary: Dict[str, Any]):
        """æ‰“å°è¯„ä¼°æ‘˜è¦æŠ¥å‘Š"""
        if not summary:
            return
            
        info = summary.get('evaluation_info', {})
        performance = summary.get('overall_performance', {})
        
        print(f"ğŸ“Š è¯„ä¼°æ‘˜è¦:")
        print(f"   æ€»æ¡ˆä¾‹æ•°: {info.get('total_cases', 0)}")
        print(f"   è¯„ä¼°è€—æ—¶: {info.get('evaluation_time', 0):.1f}ç§’")
        print()
        
        print(f"ğŸ¯ æ•´ä½“æ€§èƒ½:")
        if 'avg_overall_score' in performance:
            print(f"   ç»¼åˆè¯„åˆ†: {performance['avg_overall_score']:.3f} Â± {performance.get('std_overall_score', 0):.3f}")
            print(f"   ä¸Šä¸‹æ–‡ç›¸å…³æ€§: {performance.get('avg_context_relevance', 0):.3f}")
            print(f"   ç­”æ¡ˆå¿ å®åº¦: {performance.get('avg_faithfulness', 0):.3f}")
            print(f"   ç­”æ¡ˆç›¸å…³æ€§: {performance.get('avg_answer_relevance', 0):.3f}")
        print()
        
        # æŒ‰éš¾åº¦ç»Ÿè®¡
        difficulty_stats = summary.get('difficulty_breakdown', {})
        if difficulty_stats:
            print(f"ğŸ“ˆ æŒ‰éš¾åº¦åˆ†ç»„:")
            for difficulty, stats in difficulty_stats.items():
                print(f"   {difficulty}: {stats['avg_score']:.3f} ({stats['count']} æ¡ˆä¾‹)")
        print()
        
        # æœ€ä½³/æœ€å·®æ¡ˆä¾‹
        best_cases = summary.get('top_performing_cases', [])
        if best_cases:
            print(f"ğŸ† è¡¨ç°æœ€ä½³æ¡ˆä¾‹:")
            for case in best_cases:
                print(f"   - {case['case_id']}: {case['overall_score']:.3f}")
        print()

# ä½¿ç”¨ç¤ºä¾‹
def test_rag_evaluator():
    """æµ‹è¯•RAGè¯„ä¼°å™¨"""
    print("ğŸ§ª æµ‹è¯•RAGè¯„ä¼°å™¨...")
    
    # æ¨¡æ‹ŸRAGç³»ç»Ÿ
    class MockRAGSystem:
        def query(self, question: str) -> Dict[str, Any]:
            return {
                'answer': f"This is a sample answer for: {question}",
                'contexts': [
                    "Sample context 1 related to the question.",
                    "Sample context 2 providing additional information."
                ]
            }
    
    # åˆå§‹åŒ–è¯„ä¼°å™¨
    evaluator = RAGEvaluator()
    
    # åˆ›å»ºæµ‹è¯•æ•°æ®é›†
    dataset_manager = BenchmarkDatasets()
    test_dataset = dataset_manager.create_sample_dataset()
    
    # è¯„ä¼°RAGç³»ç»Ÿ
    mock_rag = MockRAGSystem()
    results = evaluator.evaluate_rag_system(
        rag_system=mock_rag,
        evaluation_dataset=test_dataset,
        use_llm_evaluation=False  # æµ‹è¯•æ—¶å…³é—­LLMè¯„ä¼°
    )
    
    return results

if __name__ == "__main__":
    test_rag_evaluator()