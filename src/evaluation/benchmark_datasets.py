# src/evaluation/benchmark_datasets.py
"""
è¯„ä¼°åŸºå‡†æ•°æ®é›†ç®¡ç†
ç”Ÿæˆå’Œç®¡ç†ç”¨äºRAGç³»ç»Ÿè¯„ä¼°çš„é—®ç­”æ•°æ®é›†
"""

import json
import random
from typing import List, Dict, Tuple, Optional
from dataclasses import dataclass
from pathlib import Path
import requests

@dataclass
class EvaluationCase:
    """è¯„ä¼°æ¡ˆä¾‹æ•°æ®ç»“æ„"""
    id: str
    question: str
    contexts: List[str]          # æ£€ç´¢åˆ°çš„ä¸Šä¸‹æ–‡
    ground_truth_answer: str     # æ ‡å‡†ç­”æ¡ˆ
    relevant_contexts: List[str] # ç›¸å…³çš„ä¸Šä¸‹æ–‡ï¼ˆç”¨äºå¬å›ç‡è®¡ç®—ï¼‰
    difficulty: str              # éš¾åº¦ç­‰çº§: easy, medium, hard
    category: str                # é—®é¢˜ç±»åˆ«
    metadata: Dict               # é¢å¤–å…ƒæ•°æ®

class BenchmarkDatasets:
    """åŸºå‡†æ•°æ®é›†ç®¡ç†å™¨"""
    
    def __init__(self, data_dir: str = "data/evaluation"):
        """
        åˆå§‹åŒ–æ•°æ®é›†ç®¡ç†å™¨
        
        Args:
            data_dir: è¯„ä¼°æ•°æ®å­˜å‚¨ç›®å½•
        """
        self.data_dir = Path(data_dir)
        self.data_dir.mkdir(exist_ok=True, parents=True)
        print(f"âœ… åˆå§‹åŒ–è¯„ä¼°æ•°æ®é›†ç®¡ç†å™¨: {data_dir}")
    
    def generate_academic_qa_dataset(self, papers_data_path: str, 
                                   num_questions: int = 100,
                                   llm_base_url: str = "http://localhost:11434",
                                   model: str = "llama3.1:8b") -> List[EvaluationCase]:
        """
        åŸºäºå­¦æœ¯è®ºæ–‡ç”Ÿæˆé—®ç­”è¯„ä¼°æ•°æ®é›†
        
        Args:
            papers_data_path: è®ºæ–‡æ•°æ®æ–‡ä»¶è·¯å¾„
            num_questions: ç”Ÿæˆé—®é¢˜æ•°é‡
            llm_base_url: LLMæœåŠ¡URL
            model: LLMæ¨¡å‹åç§°
            
        Returns:
            List[EvaluationCase]: è¯„ä¼°æ¡ˆä¾‹åˆ—è¡¨
        """
        print(f"ğŸ”„ ç”Ÿæˆå­¦æœ¯é—®ç­”æ•°æ®é›†: {num_questions} ä¸ªé—®é¢˜...")
        
        # åŠ è½½è®ºæ–‡æ•°æ®
        with open(papers_data_path, 'r', encoding='utf-8') as f:
            papers_data = json.load(f)
        
        evaluation_cases = []
        
        for i in range(num_questions):
            print(f"   ç”Ÿæˆé—®é¢˜ {i+1}/{num_questions}...")
            
            # éšæœºé€‰æ‹©ä¸€ç¯‡è®ºæ–‡
            paper = random.choice(papers_data)
            
            # é€‰æ‹©è®ºæ–‡çš„ä¸€äº›æ–‡æ¡£å—ä½œä¸ºä¸Šä¸‹æ–‡
            chunks = paper.get('processed_chunks', [])
            if not chunks:
                continue
                
            # éšæœºé€‰æ‹©2-4ä¸ªæ–‡æ¡£å—
            num_chunks = min(random.randint(2, 4), len(chunks))
            selected_chunks = random.sample(chunks, num_chunks)
            
            # æå–ä¸Šä¸‹æ–‡æ–‡æœ¬
            contexts = [chunk['text'] for chunk in selected_chunks]
            paper_title = paper.get('title', 'Unknown Paper')
            
            # ä½¿ç”¨LLMç”Ÿæˆé—®é¢˜å’Œç­”æ¡ˆ
            qa_pair = self._generate_qa_with_llm(
                paper_title, contexts, llm_base_url, model
            )
            
            if qa_pair:
                # ç¡®å®šé—®é¢˜éš¾åº¦å’Œç±»åˆ«
                difficulty = self._determine_difficulty(qa_pair['question'])
                category = self._determine_category(qa_pair['question'])
                
                case = EvaluationCase(
                    id=f"academic_qa_{i+1}",
                    question=qa_pair['question'],
                    contexts=contexts,
                    ground_truth_answer=qa_pair['answer'],
                    relevant_contexts=contexts,  # å‡è®¾é€‰æ‹©çš„ä¸Šä¸‹æ–‡éƒ½æ˜¯ç›¸å…³çš„
                    difficulty=difficulty,
                    category=category,
                    metadata={
                        'paper_id': paper.get('id', ''),
                        'paper_title': paper_title,
                        'chunk_count': len(contexts)
                    }
                )
                
                evaluation_cases.append(case)
                
            # é¿å…è¯·æ±‚è¿‡äºé¢‘ç¹
            import time
            time.sleep(1)
        
        print(f"âœ… æˆåŠŸç”Ÿæˆ {len(evaluation_cases)} ä¸ªè¯„ä¼°æ¡ˆä¾‹")
        return evaluation_cases
    
    def _generate_qa_with_llm(self, paper_title: str, contexts: List[str], 
                             llm_base_url: str, model: str) -> Optional[Dict]:
        """ä½¿ç”¨LLMåŸºäºè®ºæ–‡å†…å®¹ç”Ÿæˆé—®ç­”å¯¹"""
        
        combined_context = "\n\n".join(contexts)
        
        prompt = f"""
åŸºäºä»¥ä¸‹å­¦æœ¯è®ºæ–‡å†…å®¹ï¼Œç”Ÿæˆä¸€ä¸ªé«˜è´¨é‡çš„é—®ç­”å¯¹ã€‚é—®é¢˜åº”è¯¥å…·æœ‰æŒ‘æˆ˜æ€§ï¼Œç­”æ¡ˆåº”è¯¥å®Œå…¨åŸºäºç»™å®šçš„ä¸Šä¸‹æ–‡ã€‚

è®ºæ–‡æ ‡é¢˜: {paper_title}

è®ºæ–‡å†…å®¹:
{combined_context}

è¯·ç”Ÿæˆï¼š
1. ä¸€ä¸ªå…·ä½“ã€æœ‰æŒ‘æˆ˜æ€§çš„é—®é¢˜ï¼ˆé¿å…è¿‡äºå®½æ³›çš„é—®é¢˜ï¼‰
2. ä¸€ä¸ªè¯¦ç»†ã€å‡†ç¡®çš„ç­”æ¡ˆï¼ˆå®Œå…¨åŸºäºä¸Šè¿°å†…å®¹ï¼‰

è¿”å›æ ¼å¼ï¼š
{{
  "question": "<é—®é¢˜>",
  "answer": "<ç­”æ¡ˆ>"
}}
"""
        
        try:
            response = requests.post(
                f"{llm_base_url}/api/generate",
                json={
                    "model": model,
                    "prompt": prompt,
                    "stream": False,
                    "options": {
                        "temperature": 0.7,
                        "top_p": 0.9,
                        "max_tokens": 500
                    }
                },
                timeout=60
            )
            
            if response.status_code == 200:
                result = response.json()
                answer_text = result.get('response', '')
                
                # å°è¯•è§£æJSON
                try:
                    json_start = answer_text.find('{')
                    json_end = answer_text.rfind('}') + 1
                    if json_start != -1 and json_end != 0:
                        json_str = answer_text[json_start:json_end]
                        qa_pair = json.loads(json_str)
                        
                        # éªŒè¯é—®ç­”å¯¹è´¨é‡
                        if (qa_pair.get('question') and qa_pair.get('answer') and
                            len(qa_pair['question']) > 10 and len(qa_pair['answer']) > 20):
                            return qa_pair
                except:
                    pass
                    
        except Exception as e:
            print(f"      âš ï¸ LLMç”Ÿæˆå¤±è´¥: {e}")
            
        return None
    
    def _determine_difficulty(self, question: str) -> str:
        """æ ¹æ®é—®é¢˜å†…å®¹åˆ¤æ–­éš¾åº¦ç­‰çº§"""
        question_lower = question.lower()
        
        # ç®€å•é—®é¢˜å…³é”®è¯
        easy_indicators = ['what is', 'define', 'list', 'name', 'who', 'when', 'where']
        
        # å›°éš¾é—®é¢˜å…³é”®è¯  
        hard_indicators = ['compare', 'analyze', 'evaluate', 'why', 'how does', 'what are the implications', 'discuss']
        
        if any(indicator in question_lower for indicator in hard_indicators):
            return 'hard'
        elif any(indicator in question_lower for indicator in easy_indicators):
            return 'easy'
        else:
            return 'medium'
    
    def _determine_category(self, question: str) -> str:
        """æ ¹æ®é—®é¢˜å†…å®¹åˆ¤æ–­ç±»åˆ«"""
        question_lower = question.lower()
        
        categories = {
            'definition': ['what is', 'define', 'definition of'],
            'method': ['how', 'method', 'approach', 'algorithm', 'technique'],
            'comparison': ['compare', 'difference', 'vs', 'versus', 'better'],
            'application': ['application', 'use', 'apply', 'implement'],
            'evaluation': ['performance', 'result', 'accuracy', 'evaluation', 'experiment'],
            'conceptual': ['why', 'reason', 'principle', 'concept', 'theory']
        }
        
        for category, keywords in categories.items():
            if any(keyword in question_lower for keyword in keywords):
                return category
                
        return 'general'
    
    def create_custom_dataset(self, cases: List[Dict]) -> List[EvaluationCase]:
        """
        ä»è‡ªå®šä¹‰æ•°æ®åˆ›å»ºè¯„ä¼°æ•°æ®é›†
        
        Args:
            cases: è‡ªå®šä¹‰æ¡ˆä¾‹åˆ—è¡¨
            
        Returns:
            List[EvaluationCase]: è¯„ä¼°æ¡ˆä¾‹åˆ—è¡¨
        """
        evaluation_cases = []
        
        for i, case in enumerate(cases):
            eval_case = EvaluationCase(
                id=case.get('id', f"custom_{i+1}"),
                question=case['question'],
                contexts=case['contexts'],
                ground_truth_answer=case['ground_truth_answer'],
                relevant_contexts=case.get('relevant_contexts', case['contexts']),
                difficulty=case.get('difficulty', 'medium'),
                category=case.get('category', 'general'),
                metadata=case.get('metadata', {})
            )
            evaluation_cases.append(eval_case)
            
        return evaluation_cases
    
    def save_dataset(self, dataset: List[EvaluationCase], filename: str):
        """ä¿å­˜æ•°æ®é›†åˆ°æ–‡ä»¶"""
        dataset_data = []
        for case in dataset:
            case_dict = {
                'id': case.id,
                'question': case.question,
                'contexts': case.contexts,
                'ground_truth_answer': case.ground_truth_answer,
                'relevant_contexts': case.relevant_contexts,
                'difficulty': case.difficulty,
                'category': case.category,
                'metadata': case.metadata
            }
            dataset_data.append(case_dict)
        
        save_path = self.data_dir / filename
        with open(save_path, 'w', encoding='utf-8') as f:
            json.dump(dataset_data, f, ensure_ascii=False, indent=2)
            
        print(f"âœ… æ•°æ®é›†å·²ä¿å­˜: {save_path}")
    
    def load_dataset(self, filename: str) -> List[EvaluationCase]:
        """ä»æ–‡ä»¶åŠ è½½æ•°æ®é›†"""
        load_path = self.data_dir / filename
        
        if not load_path.exists():
            raise FileNotFoundError(f"æ•°æ®é›†æ–‡ä»¶ä¸å­˜åœ¨: {load_path}")
        
        with open(load_path, 'r', encoding='utf-8') as f:
            dataset_data = json.load(f)
        
        evaluation_cases = []
        for case_dict in dataset_data:
            case = EvaluationCase(
                id=case_dict['id'],
                question=case_dict['question'],
                contexts=case_dict['contexts'],
                ground_truth_answer=case_dict['ground_truth_answer'],
                relevant_contexts=case_dict['relevant_contexts'],
                difficulty=case_dict['difficulty'],
                category=case_dict['category'],
                metadata=case_dict['metadata']
            )
            evaluation_cases.append(case)
        
        print(f"âœ… æ•°æ®é›†å·²åŠ è½½: {load_path} ({len(evaluation_cases)} ä¸ªæ¡ˆä¾‹)")
        return evaluation_cases

    def create_sample_dataset(self) -> List[EvaluationCase]:
        """åˆ›å»ºç¤ºä¾‹è¯„ä¼°æ•°æ®é›†"""
        sample_cases = [
            {
                'id': 'sample_1',
                'question': 'What is the main contribution of the attention mechanism in neural networks?',
                'contexts': [
                    'The attention mechanism allows neural networks to focus on relevant parts of the input sequence.',
                    'Attention was introduced to solve the bottleneck problem in encoder-decoder architectures.',
                    'The mechanism computes weighted averages of input representations based on relevance scores.'
                ],
                'ground_truth_answer': 'The main contribution of the attention mechanism is allowing neural networks to selectively focus on relevant parts of the input sequence, solving the bottleneck problem in encoder-decoder architectures by computing weighted averages based on relevance scores.',
                'difficulty': 'medium',
                'category': 'conceptual'
            },
            {
                'id': 'sample_2', 
                'question': 'How does the transformer architecture differ from RNN-based models?',
                'contexts': [
                    'Transformers rely entirely on self-attention mechanisms without recurrence.',
                    'RNNs process sequences step by step, leading to sequential dependencies.',
                    'The transformer architecture enables parallel processing of all sequence positions.',
                    'Self-attention allows direct modeling of dependencies regardless of distance.'
                ],
                'ground_truth_answer': 'The transformer architecture differs from RNN-based models by relying entirely on self-attention mechanisms without recurrence, enabling parallel processing of all sequence positions and direct modeling of long-range dependencies, unlike RNNs which process sequences sequentially.',
                'difficulty': 'hard',
                'category': 'comparison'
            }
        ]
        
        return self.create_custom_dataset(sample_cases)

# ä½¿ç”¨ç¤ºä¾‹
def test_benchmark_datasets():
    """æµ‹è¯•åŸºå‡†æ•°æ®é›†åŠŸèƒ½"""
    print("ğŸ§ª æµ‹è¯•åŸºå‡†æ•°æ®é›†...")
    
    dataset_manager = BenchmarkDatasets()
    
    # åˆ›å»ºç¤ºä¾‹æ•°æ®é›†
    sample_dataset = dataset_manager.create_sample_dataset()
    
    # ä¿å­˜å’ŒåŠ è½½æ•°æ®é›†
    dataset_manager.save_dataset(sample_dataset, "sample_evaluation_dataset.json")
    loaded_dataset = dataset_manager.load_dataset("sample_evaluation_dataset.json")
    
    print(f"ğŸ“Š æ•°æ®é›†ä¿¡æ¯:")
    print(f"   æ¡ˆä¾‹æ•°é‡: {len(loaded_dataset)}")
    for case in loaded_dataset:
        print(f"   - {case.id}: {case.question[:50]}... (éš¾åº¦: {case.difficulty}, ç±»åˆ«: {case.category})")
    
    return loaded_dataset

if __name__ == "__main__":
    test_benchmark_datasets()