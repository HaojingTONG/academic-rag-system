# src/embedding/advanced_embedding.py
"""
é«˜çº§æ–‡æœ¬åµŒå…¥æ¨¡å— - æ”¯æŒå¤šç§åµŒå…¥æ¨¡å‹å’Œä¼˜åŒ–ç­–ç•¥
"""

import os
import time
import hashlib
import pickle
import numpy as np
from typing import List, Dict, Optional, Union, Tuple
from dataclasses import dataclass
from pathlib import Path
import torch
from sentence_transformers import SentenceTransformer
from abc import ABC, abstractmethod

@dataclass
class EmbeddingConfig:
    """åµŒå…¥é…ç½®"""
    model_name: str = "all-MiniLM-L6-v2"  # é»˜è®¤æ¨¡å‹
    model_type: str = "sentence_transformers"  # æ¨¡å‹ç±»å‹
    device: str = "auto"  # è®¾å¤‡é€‰æ‹©
    batch_size: int = 32  # æ‰¹å¤„ç†å¤§å°
    normalize_embeddings: bool = True  # æ˜¯å¦æ ‡å‡†åŒ–å‘é‡
    cache_enabled: bool = True  # æ˜¯å¦å¯ç”¨ç¼“å­˜
    cache_dir: str = "data/embedding_cache"  # ç¼“å­˜ç›®å½•
    max_sequence_length: int = 512  # æœ€å¤§åºåˆ—é•¿åº¦

class BaseEmbeddingModel(ABC):
    """åµŒå…¥æ¨¡å‹åŸºç±»"""
    
    def __init__(self, config: EmbeddingConfig):
        self.config = config
        self.model_info = {}
        self._setup_device()
        self._setup_cache()
        
    def _setup_device(self):
        """è®¾ç½®è®¡ç®—è®¾å¤‡"""
        if self.config.device == "auto":
            if torch.cuda.is_available():
                self.device = "cuda"
            elif torch.backends.mps.is_available():
                self.device = "mps"
            else:
                self.device = "cpu"
        else:
            self.device = self.config.device
        
        print(f"ğŸ”¥ åµŒå…¥æ¨¡å‹ä½¿ç”¨è®¾å¤‡: {self.device}")
    
    def _setup_cache(self):
        """è®¾ç½®ç¼“å­˜"""
        if self.config.cache_enabled:
            Path(self.config.cache_dir).mkdir(parents=True, exist_ok=True)
            self.cache_enabled = True
        else:
            self.cache_enabled = False
    
    def _get_cache_key(self, text: str) -> str:
        """ç”Ÿæˆç¼“å­˜é”®"""
        content = f"{self.config.model_name}:{text}"
        return hashlib.md5(content.encode()).hexdigest()
    
    def _load_from_cache(self, cache_key: str) -> Optional[np.ndarray]:
        """ä»ç¼“å­˜åŠ è½½åµŒå…¥"""
        if not self.cache_enabled:
            return None
        
        cache_file = Path(self.config.cache_dir) / f"{cache_key}.pkl"
        if cache_file.exists():
            try:
                with open(cache_file, 'rb') as f:
                    return pickle.load(f)
            except Exception:
                return None
        return None
    
    def _save_to_cache(self, cache_key: str, embedding: np.ndarray):
        """ä¿å­˜åµŒå…¥åˆ°ç¼“å­˜"""
        if not self.cache_enabled:
            return
        
        cache_file = Path(self.config.cache_dir) / f"{cache_key}.pkl"
        try:
            with open(cache_file, 'wb') as f:
                pickle.dump(embedding, f)
        except Exception as e:
            print(f"âš ï¸ ç¼“å­˜ä¿å­˜å¤±è´¥: {e}")
    
    @abstractmethod
    def encode_single(self, text: str) -> np.ndarray:
        """ç¼–ç å•ä¸ªæ–‡æœ¬"""
        pass
    
    @abstractmethod
    def encode_batch(self, texts: List[str]) -> np.ndarray:
        """æ‰¹é‡ç¼–ç æ–‡æœ¬"""
        pass
    
    def encode(self, texts: Union[str, List[str]], 
               show_progress: bool = True, 
               use_cache: bool = True) -> np.ndarray:
        """ç»Ÿä¸€ç¼–ç æ¥å£"""
        if isinstance(texts, str):
            return self.encode_single(texts)
        else:
            return self.encode_batch(texts, show_progress, use_cache)
    
    def get_model_info(self) -> Dict:
        """è·å–æ¨¡å‹ä¿¡æ¯"""
        return self.model_info

class SentenceTransformersModel(BaseEmbeddingModel):
    """Sentence Transformersæ¨¡å‹"""
    
    # æ¨èçš„æ¨¡å‹é…ç½®
    RECOMMENDED_MODELS = {
        # è‹±æ–‡é€šç”¨æ¨¡å‹
        "all-MiniLM-L6-v2": {
            "description": "è½»é‡çº§è‹±æ–‡æ¨¡å‹ï¼Œ384ç»´ï¼Œé€Ÿåº¦å¿«",
            "dimension": 384,
            "language": "en",
            "performance": "good",
            "speed": "fast"
        },
        "all-mpnet-base-v2": {
            "description": "é«˜è´¨é‡è‹±æ–‡æ¨¡å‹ï¼Œ768ç»´ï¼Œæ•ˆæœå¥½",
            "dimension": 768,
            "language": "en", 
            "performance": "excellent",
            "speed": "medium"
        },
        
        # å¤šè¯­è¨€æ¨¡å‹
        "paraphrase-multilingual-MiniLM-L12-v2": {
            "description": "å¤šè¯­è¨€è½»é‡æ¨¡å‹ï¼Œ384ç»´",
            "dimension": 384,
            "language": "multilingual",
            "performance": "good",
            "speed": "fast"
        },
        "paraphrase-multilingual-mpnet-base-v2": {
            "description": "å¤šè¯­è¨€é«˜è´¨é‡æ¨¡å‹ï¼Œ768ç»´",
            "dimension": 768,
            "language": "multilingual",
            "performance": "excellent", 
            "speed": "medium"
        },
        
        # ä¸­æ–‡ä¼˜åŒ–æ¨¡å‹
        "shibing624/text2vec-base-chinese": {
            "description": "ä¸­æ–‡ä¼˜åŒ–æ¨¡å‹ï¼Œ768ç»´",
            "dimension": 768,
            "language": "zh",
            "performance": "excellent",
            "speed": "medium"
        },
        
        # å­¦æœ¯æ–‡æœ¬ä¸“ç”¨
        "allenai/specter": {
            "description": "å­¦æœ¯è®ºæ–‡ä¸“ç”¨æ¨¡å‹ï¼Œ768ç»´",
            "dimension": 768,
            "language": "en",
            "performance": "excellent",
            "speed": "slow",
            "domain": "academic"
        }
    }
    
    def __init__(self, config: EmbeddingConfig):
        super().__init__(config)
        self._load_model()
        
    def _load_model(self):
        """åŠ è½½æ¨¡å‹"""
        print(f"ğŸ“¥ åŠ è½½åµŒå…¥æ¨¡å‹: {self.config.model_name}")
        
        try:
            self.model = SentenceTransformer(
                self.config.model_name, 
                device=self.device
            )
            
            # è®¾ç½®æœ€å¤§åºåˆ—é•¿åº¦
            if hasattr(self.model, 'max_seq_length'):
                self.model.max_seq_length = self.config.max_sequence_length
            
            # è·å–æ¨¡å‹ä¿¡æ¯
            self.model_info = {
                "model_name": self.config.model_name,
                "dimension": self.model.get_sentence_embedding_dimension(),
                "device": self.device,
                "max_sequence_length": self.config.max_sequence_length,
                "recommended_info": self.RECOMMENDED_MODELS.get(
                    self.config.model_name, {"description": "è‡ªå®šä¹‰æ¨¡å‹"}
                )
            }
            
            print(f"âœ… æ¨¡å‹åŠ è½½æˆåŠŸ")
            print(f"   - ç»´åº¦: {self.model_info['dimension']}")
            print(f"   - è®¾å¤‡: {self.device}")
            print(f"   - æè¿°: {self.model_info['recommended_info']['description']}")
            
        except Exception as e:
            print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            raise
    
    def encode_single(self, text: str) -> np.ndarray:
        """ç¼–ç å•ä¸ªæ–‡æœ¬"""
        # æ£€æŸ¥ç¼“å­˜
        cache_key = self._get_cache_key(text)
        cached_embedding = self._load_from_cache(cache_key)
        if cached_embedding is not None:
            return cached_embedding
        
        # ç”ŸæˆåµŒå…¥
        embedding = self.model.encode([text], 
                                    normalize_embeddings=self.config.normalize_embeddings,
                                    show_progress_bar=False)[0]
        
        # ä¿å­˜åˆ°ç¼“å­˜
        self._save_to_cache(cache_key, embedding)
        
        return embedding
    
    def encode_batch(self, texts: List[str], 
                    show_progress: bool = True, 
                    use_cache: bool = True) -> np.ndarray:
        """æ‰¹é‡ç¼–ç æ–‡æœ¬"""
        if not texts:
            return np.array([])
        
        # æ£€æŸ¥ç¼“å­˜
        embeddings = []
        texts_to_encode = []
        cache_keys = []
        indices_to_encode = []
        
        if use_cache and self.cache_enabled:
            for i, text in enumerate(texts):
                cache_key = self._get_cache_key(text)
                cached_embedding = self._load_from_cache(cache_key)
                if cached_embedding is not None:
                    embeddings.append((i, cached_embedding))
                else:
                    texts_to_encode.append(text)
                    cache_keys.append(cache_key)
                    indices_to_encode.append(i)
        else:
            texts_to_encode = texts
            indices_to_encode = list(range(len(texts)))
        
        # æ‰¹é‡ç”Ÿæˆæ–°çš„åµŒå…¥
        if texts_to_encode:
            if show_progress:
                print(f"ğŸ”¢ ç”Ÿæˆ {len(texts_to_encode)} ä¸ªæ–°åµŒå…¥å‘é‡...")
            
            # åˆ†æ‰¹å¤„ç†é¿å…å†…å­˜é—®é¢˜
            batch_embeddings = []
            batch_size = self.config.batch_size
            
            for i in range(0, len(texts_to_encode), batch_size):
                batch_texts = texts_to_encode[i:i + batch_size]
                batch_emb = self.model.encode(
                    batch_texts,
                    normalize_embeddings=self.config.normalize_embeddings,
                    show_progress_bar=show_progress and len(texts_to_encode) > batch_size,
                    batch_size=min(batch_size, len(batch_texts))
                )
                batch_embeddings.append(batch_emb)
            
            # åˆå¹¶æ‰¹æ¬¡ç»“æœ
            if batch_embeddings:
                new_embeddings = np.vstack(batch_embeddings)
                
                # æ·»åŠ åˆ°ç»“æœå’Œç¼“å­˜
                for idx, embedding, cache_key in zip(indices_to_encode, new_embeddings, cache_keys):
                    embeddings.append((idx, embedding))
                    if use_cache:
                        self._save_to_cache(cache_key, embedding)
        
        # æŒ‰åŸå§‹é¡ºåºæ’åº
        embeddings.sort(key=lambda x: x[0])
        result = np.array([emb for _, emb in embeddings])
        
        return result

class HybridEmbeddingModel(BaseEmbeddingModel):
    """æ··åˆåµŒå…¥æ¨¡å‹ - ç»“åˆå¤šä¸ªæ¨¡å‹çš„ä¼˜åŠ¿"""
    
    def __init__(self, config: EmbeddingConfig, models: List[str] = None):
        super().__init__(config)
        
        # é»˜è®¤æ¨¡å‹ç»„åˆ
        if models is None:
            models = [
                "all-MiniLM-L6-v2",  # å¿«é€Ÿæ¨¡å‹
                "all-mpnet-base-v2"   # é«˜è´¨é‡æ¨¡å‹
            ]
        
        self.models = []
        self.weights = []  # æ¨¡å‹æƒé‡
        
        # åŠ è½½å¤šä¸ªæ¨¡å‹
        for model_name in models:
            model_config = EmbeddingConfig(
                model_name=model_name,
                device=self.device,
                batch_size=self.config.batch_size // len(models),  # åˆ†é…æ‰¹æ¬¡å¤§å°
                cache_enabled=self.config.cache_enabled,
                cache_dir=f"{self.config.cache_dir}/{model_name}"
            )
            model = SentenceTransformersModel(model_config)
            self.models.append(model)
            
            # æ ¹æ®æ¨¡å‹æ€§èƒ½è®¾ç½®æƒé‡
            model_info = model.RECOMMENDED_MODELS.get(model_name, {})
            if model_info.get("performance") == "excellent":
                self.weights.append(0.7)
            else:
                self.weights.append(0.3)
        
        # æ ‡å‡†åŒ–æƒé‡
        total_weight = sum(self.weights)
        self.weights = [w / total_weight for w in self.weights]
        
        # è®¡ç®—æ··åˆç»´åº¦
        dimensions = [model.model_info["dimension"] for model in self.models]
        self.model_info = {
            "model_names": models,
            "dimension": sum(dimensions),  # è¿æ¥æ‰€æœ‰ç»´åº¦
            "individual_dimensions": dimensions,
            "weights": self.weights,
            "device": self.device
        }
        
        print(f"ğŸ”— æ··åˆåµŒå…¥æ¨¡å‹åˆå§‹åŒ–å®Œæˆ")
        print(f"   - æ¨¡å‹æ•°é‡: {len(self.models)}")
        print(f"   - æ€»ç»´åº¦: {self.model_info['dimension']}")
        print(f"   - æƒé‡åˆ†é…: {self.weights}")
    
    def encode_single(self, text: str) -> np.ndarray:
        """ç¼–ç å•ä¸ªæ–‡æœ¬"""
        embeddings = []
        for model in self.models:
            emb = model.encode_single(text)
            embeddings.append(emb)
        
        # è¿æ¥æ‰€æœ‰åµŒå…¥
        return np.concatenate(embeddings)
    
    def encode_batch(self, texts: List[str], 
                    show_progress: bool = True, 
                    use_cache: bool = True) -> np.ndarray:
        """æ‰¹é‡ç¼–ç æ–‡æœ¬"""
        all_embeddings = []
        
        for i, model in enumerate(self.models):
            if show_progress:
                print(f"ğŸ”„ ä½¿ç”¨æ¨¡å‹ {i+1}/{len(self.models)}: {model.config.model_name}")
            
            embeddings = model.encode_batch(texts, show_progress, use_cache)
            all_embeddings.append(embeddings)
        
        # è¿æ¥æ‰€æœ‰åµŒå…¥
        return np.concatenate(all_embeddings, axis=1)

class EmbeddingManager:
    """åµŒå…¥ç®¡ç†å™¨ - ç»Ÿä¸€ç®¡ç†åµŒå…¥åŠŸèƒ½"""
    
    def __init__(self, config: EmbeddingConfig = None):
        self.config = config or EmbeddingConfig()
        self.model = None
        self.stats = {
            "total_encoded": 0,
            "cache_hits": 0,
            "encoding_time": 0.0
        }
        
    def initialize(self, model_type: str = None):
        """åˆå§‹åŒ–åµŒå…¥æ¨¡å‹"""
        model_type = model_type or self.config.model_type
        
        print(f"ğŸš€ åˆå§‹åŒ–åµŒå…¥ç®¡ç†å™¨...")
        print(f"   - æ¨¡å‹ç±»å‹: {model_type}")
        print(f"   - æ¨¡å‹åç§°: {self.config.model_name}")
        
        if model_type == "sentence_transformers":
            self.model = SentenceTransformersModel(self.config)
        elif model_type == "hybrid":
            self.model = HybridEmbeddingModel(self.config)
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„æ¨¡å‹ç±»å‹: {model_type}")
        
        print(f"âœ… åµŒå…¥ç®¡ç†å™¨åˆå§‹åŒ–å®Œæˆ")
        
    def encode(self, texts: Union[str, List[str]], **kwargs) -> np.ndarray:
        """ç¼–ç æ–‡æœ¬"""
        if self.model is None:
            self.initialize()
        
        start_time = time.time()
        
        # æ‰§è¡Œç¼–ç 
        embeddings = self.model.encode(texts, **kwargs)
        
        # æ›´æ–°ç»Ÿè®¡
        encoding_time = time.time() - start_time
        self.stats["encoding_time"] += encoding_time
        self.stats["total_encoded"] += len(texts) if isinstance(texts, list) else 1
        
        return embeddings
    
    def get_model_info(self) -> Dict:
        """è·å–æ¨¡å‹ä¿¡æ¯"""
        if self.model is None:
            return {}
        return self.model.get_model_info()
    
    def get_stats(self) -> Dict:
        """è·å–ç»Ÿè®¡ä¿¡æ¯"""
        stats = self.stats.copy()
        stats["avg_encoding_time"] = (
            stats["encoding_time"] / max(stats["total_encoded"], 1)
        )
        return stats
    
    def clear_cache(self):
        """æ¸…é™¤ç¼“å­˜"""
        if self.config.cache_enabled:
            import shutil
            cache_path = Path(self.config.cache_dir)
            if cache_path.exists():
                shutil.rmtree(cache_path)
                cache_path.mkdir(parents=True)
                print("ğŸ—‘ï¸ åµŒå…¥ç¼“å­˜å·²æ¸…é™¤")
    
    @staticmethod
    def get_recommended_models() -> Dict:
        """è·å–æ¨èæ¨¡å‹åˆ—è¡¨"""
        return SentenceTransformersModel.RECOMMENDED_MODELS
    
    @staticmethod
    def benchmark_models(test_texts: List[str], 
                        models: List[str] = None) -> Dict:
        """åŸºå‡†æµ‹è¯•ä¸åŒæ¨¡å‹"""
        if models is None:
            models = [
                "all-MiniLM-L6-v2",
                "all-mpnet-base-v2", 
                "paraphrase-multilingual-MiniLM-L12-v2"
            ]
        
        results = {}
        
        print("ğŸ”¬ å¼€å§‹åµŒå…¥æ¨¡å‹åŸºå‡†æµ‹è¯•...")
        
        for model_name in models:
            print(f"\næµ‹è¯•æ¨¡å‹: {model_name}")
            
            try:
                config = EmbeddingConfig(
                    model_name=model_name,
                    cache_enabled=False  # ç¦ç”¨ç¼“å­˜ä»¥è·å¾—çœŸå®æ€§èƒ½
                )
                
                manager = EmbeddingManager(config)
                manager.initialize()
                
                # æ€§èƒ½æµ‹è¯•
                start_time = time.time()
                embeddings = manager.encode(test_texts, show_progress=False)
                encoding_time = time.time() - start_time
                
                # è·å–æ¨¡å‹ä¿¡æ¯
                model_info = manager.get_model_info()
                
                results[model_name] = {
                    "success": True,
                    "encoding_time": encoding_time,
                    "avg_time_per_text": encoding_time / len(test_texts),
                    "dimension": model_info["dimension"],
                    "model_info": model_info.get("recommended_info", {}),
                    "embeddings_shape": embeddings.shape
                }
                
                print(f"âœ… {model_name} æµ‹è¯•å®Œæˆ")
                print(f"   - è€—æ—¶: {encoding_time:.2f}ç§’")
                print(f"   - ç»´åº¦: {model_info['dimension']}")
                
            except Exception as e:
                results[model_name] = {
                    "success": False,
                    "error": str(e)
                }
                print(f"âŒ {model_name} æµ‹è¯•å¤±è´¥: {e}")
        
        return results

# ä½¿ç”¨ç¤ºä¾‹å’Œæµ‹è¯•å‡½æ•°
def test_embedding_module():
    """æµ‹è¯•åµŒå…¥æ¨¡å—"""
    print("ğŸ§ª æµ‹è¯•é«˜çº§åµŒå…¥æ¨¡å—")
    print("=" * 60)
    
    # æµ‹è¯•æ–‡æœ¬
    test_texts = [
        "Transformer architecture with attention mechanism",
        "Deep learning for natural language processing", 
        "Neural networks and machine learning algorithms",
        "Artificial intelligence and computer vision"
    ]
    
    # æµ‹è¯•ä¸åŒé…ç½®
    configs = [
        {
            "model_name": "all-MiniLM-L6-v2",
            "model_type": "sentence_transformers",
            "description": "è½»é‡çº§æ¨¡å‹"
        },
        {
            "model_name": "all-mpnet-base-v2", 
            "model_type": "sentence_transformers",
            "description": "é«˜è´¨é‡æ¨¡å‹"
        }
    ]
    
    for config in configs:
        print(f"\nğŸ” æµ‹è¯• {config['description']}")
        print("-" * 40)
        
        embedding_config = EmbeddingConfig(
            model_name=config["model_name"],
            model_type=config["model_type"],
            cache_enabled=True
        )
        
        manager = EmbeddingManager(embedding_config)
        manager.initialize()
        
        # ç¼–ç æµ‹è¯•
        embeddings = manager.encode(test_texts)
        model_info = manager.get_model_info()
        stats = manager.get_stats()
        
        print(f"ğŸ“Š ç»“æœ:")
        print(f"   - åµŒå…¥ç»´åº¦: {embeddings.shape}")
        print(f"   - æ¨¡å‹ç»´åº¦: {model_info['dimension']}")
        print(f"   - ç¼–ç æ—¶é—´: {stats['encoding_time']:.3f}ç§’")
        print(f"   - å¹³å‡æ—¶é—´: {stats['avg_encoding_time']:.3f}ç§’/æ–‡æœ¬")

if __name__ == "__main__":
    test_embedding_module()