[
  {
    "id": "2106.07932v4",
    "title": "Medical Code Prediction from Discharge Summary: Document to Sequence BERT using Sequence Attention",
    "authors": [
      "Tak-Sung Heo",
      "Yongmin Yoo",
      "Yeongjoon Park",
      "Byeong-Cheol Jo",
      "Kyungsun Kim"
    ],
    "abstract": "Clinical notes are unstructured text generated by clinicians during patient\nencounters. Clinical notes are usually accompanied by a set of metadata codes\nfrom the International Classification of Diseases(ICD). ICD code is an\nimportant code used in various operations, including insurance, reimbursement,\nmedical diagnosis, etc. Therefore, it is important to classify ICD codes\nquickly and accurately. However, annotating these codes is costly and\ntime-consuming. So we propose a model based on bidirectional encoder\nrepresentations from transformers (BERT) using the sequence attention method\nfor automatic ICD code assignment. We evaluate our approach on the medical\ninformation mart for intensive care III (MIMIC-III) benchmark dataset. Our\nmodel achieved performance of macro-averaged F1: 0.62898 and micro-averaged F1:\n0.68555 and is performing better than a performance of the state-of-the-art\nmodel using the MIMIC-III dataset. The contribution of this study proposes a\nmethod of using BERT that can be applied to documents and a sequence attention\nmethod that can capture important sequence in-formation appearing in documents.",
    "published": "2021-06-15",
    "pdf_url": "http://arxiv.org/pdf/2106.07932v4",
    "local_path": "data/raw_papers/2106.07932v4.pdf"
  },
  {
    "id": "2410.15198v4",
    "title": "Medical-GAT: Cancer Document Classification Leveraging Graph-Based Residual Network for Scenarios with Limited Data",
    "authors": [
      "Elias Hossain",
      "Tasfia Nuzhat",
      "Shamsul Masum",
      "Shahram Rahimi",
      "Noorbakhsh Amiri Golilarz"
    ],
    "abstract": "Accurate classification of cancer-related medical abstracts is crucial for\nhealthcare management and research. However, obtaining large, labeled datasets\nin the medical domain is challenging due to privacy concerns and the complexity\nof clinical data. This scarcity of annotated data impedes the development of\neffective machine learning models for cancer document classification. To\naddress this challenge, we present a curated dataset of 1,874 biomedical\nabstracts, categorized into thyroid cancer, colon cancer, lung cancer, and\ngeneric topics. Our research focuses on leveraging this dataset to improve\nclassification performance, particularly in data-scarce scenarios. We introduce\na Residual Graph Attention Network (R-GAT) with multiple graph attention layers\nthat capture the semantic information and structural relationships within\ncancer-related documents. Our R-GAT model is compared with various techniques,\nincluding transformer-based models such as Bidirectional Encoder\nRepresentations from Transformers (BERT), RoBERTa, and domain-specific models\nlike BioBERT and Bio+ClinicalBERT. We also evaluated deep learning models\n(CNNs, LSTMs) and traditional machine learning models (Logistic Regression,\nSVM). Additionally, we explore ensemble approaches that combine deep learning\nmodels to enhance classification. Various feature extraction methods are\nassessed, including Term Frequency-Inverse Document Frequency (TF-IDF) with\nunigrams and bigrams, Word2Vec, and tokenizers from BERT and RoBERTa. The R-GAT\nmodel outperforms other techniques, achieving precision, recall, and F1 scores\nof 0.99, 0.97, and 0.98 for thyroid cancer; 0.96, 0.94, and 0.95 for colon\ncancer; 0.96, 0.99, and 0.97 for lung cancer; and 0.95, 0.96, and 0.95 for\ngeneric topics.",
    "published": "2024-10-19",
    "pdf_url": "http://arxiv.org/pdf/2410.15198v4",
    "local_path": "data/raw_papers/2410.15198v4.pdf"
  },
  {
    "id": "2010.07523v2",
    "title": "Context-Guided BERT for Targeted Aspect-Based Sentiment Analysis",
    "authors": [
      "Zhengxuan Wu",
      "Desmond C. Ong"
    ],
    "abstract": "Aspect-based sentiment analysis (ABSA) and Targeted ASBA (TABSA) allow\nfiner-grained inferences about sentiment to be drawn from the same text,\ndepending on context. For example, a given text can have different targets\n(e.g., neighborhoods) and different aspects (e.g., price or safety), with\ndifferent sentiment associated with each target-aspect pair. In this paper, we\ninvestigate whether adding context to self-attention models improves\nperformance on (T)ABSA. We propose two variants of Context-Guided BERT\n(CG-BERT) that learn to distribute attention under different contexts. We first\nadapt a context-aware Transformer to produce a CG-BERT that uses context-guided\nsoftmax-attention. Next, we propose an improved Quasi-Attention CG-BERT model\nthat learns a compositional attention that supports subtractive attention. We\ntrain both models with pretrained BERT on two (T)ABSA datasets: SentiHood and\nSemEval-2014 (Task 4). Both models achieve new state-of-the-art results with\nour QACG-BERT model having the best performance. Furthermore, we provide\nanalyses of the impact of context in the our proposed models. Our work provides\nmore evidence for the utility of adding context-dependencies to pretrained\nself-attention-based language models for context-based natural language tasks.",
    "published": "2020-10-15",
    "pdf_url": "http://arxiv.org/pdf/2010.07523v2",
    "local_path": "data/raw_papers/2010.07523v2.pdf"
  },
  {
    "id": "2009.14409v1",
    "title": "AUBER: Automated BERT Regularization",
    "authors": [
      "Hyun Dong Lee",
      "Seongmin Lee",
      "U Kang"
    ],
    "abstract": "How can we effectively regularize BERT? Although BERT proves its\neffectiveness in various downstream natural language processing tasks, it often\noverfits when there are only a small number of training instances. A promising\ndirection to regularize BERT is based on pruning its attention heads based on a\nproxy score for head importance. However, heuristic-based methods are usually\nsuboptimal since they predetermine the order by which attention heads are\npruned. In order to overcome such a limitation, we propose AUBER, an effective\nregularization method that leverages reinforcement learning to automatically\nprune attention heads from BERT. Instead of depending on heuristics or\nrule-based policies, AUBER learns a pruning policy that determines which\nattention heads should or should not be pruned for regularization. Experimental\nresults show that AUBER outperforms existing pruning methods by achieving up to\n10% better accuracy. In addition, our ablation study empirically demonstrates\nthe effectiveness of our design choices for AUBER.",
    "published": "2020-09-30",
    "pdf_url": "http://arxiv.org/pdf/2009.14409v1",
    "local_path": "data/raw_papers/2009.14409v1.pdf"
  },
  {
    "id": "2102.10934v1",
    "title": "Using Prior Knowledge to Guide BERT's Attention in Semantic Textual Matching Tasks",
    "authors": [
      "Tingyu Xia",
      "Yue Wang",
      "Yuan Tian",
      "Yi Chang"
    ],
    "abstract": "We study the problem of incorporating prior knowledge into a deep\nTransformer-based model,i.e.,Bidirectional Encoder Representations from\nTransformers (BERT), to enhance its performance on semantic textual matching\ntasks. By probing and analyzing what BERT has already known when solving this\ntask, we obtain better understanding of what task-specific knowledge BERT needs\nthe most and where it is most needed. The analysis further motivates us to take\na different approach than most existing works. Instead of using prior knowledge\nto create a new training task for fine-tuning BERT, we directly inject\nknowledge into BERT's multi-head attention mechanism. This leads us to a simple\nyet effective approach that enjoys fast training stage as it saves the model\nfrom training on additional data or tasks other than the main task. Extensive\nexperiments demonstrate that the proposed knowledge-enhanced BERT is able to\nconsistently improve semantic textual matching performance over the original\nBERT model, and the performance benefit is most salient when training data is\nscarce.",
    "published": "2021-02-22",
    "pdf_url": "http://arxiv.org/pdf/2102.10934v1",
    "local_path": "data/raw_papers/2102.10934v1.pdf"
  }
]