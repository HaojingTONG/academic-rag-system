#!/usr/bin/env python3
"""
RAGç³»ç»Ÿè¯„ä¼°è„šæœ¬
ä½¿ç”¨è¯„ä¼°æ¡†æ¶å¯¹å­¦æœ¯è®ºæ–‡RAGç³»ç»Ÿè¿›è¡Œå®Œæ•´è¯„ä¼°
"""

import sys
from pathlib import Path
import json

# æ·»åŠ srcåˆ°è·¯å¾„
sys.path.append(str(Path(__file__).parent))
sys.path.append(str(Path(__file__).parent / 'src'))

from src.evaluation import RAGEvaluator, BenchmarkDatasets
from main_rag_system import AcademicRAGSystem

def main():
    """ä¸»è¯„ä¼°æµç¨‹"""
    print("ğŸš€ RAGç³»ç»Ÿè¯„ä¼°å¼€å§‹")
    print("=" * 80)
    
    # 1. åˆå§‹åŒ–RAGç³»ç»Ÿ
    print("ğŸ“š åˆå§‹åŒ–RAGç³»ç»Ÿ...")
    rag_system = AcademicRAGSystem()
    
    # 2. åˆå§‹åŒ–è¯„ä¼°å™¨
    print("ğŸ“Š åˆå§‹åŒ–è¯„ä¼°å™¨...")
    evaluator = RAGEvaluator(
        embedding_model="all-MiniLM-L6-v2",
        llm_base_url="http://localhost:11434",
        llm_model="llama3.1:8b",
        output_dir="data/evaluation/results"
    )
    
    # 3. å‡†å¤‡è¯„ä¼°æ•°æ®é›†
    print("ğŸ“‹ å‡†å¤‡è¯„ä¼°æ•°æ®é›†...")
    dataset_manager = BenchmarkDatasets("data/evaluation")
    
    # é€‰æ‹©è¯„ä¼°æ•°æ®é›†åˆ›å»ºæ–¹å¼
    choice = input("é€‰æ‹©æ•°æ®é›†åˆ›å»ºæ–¹å¼:\n1. ä½¿ç”¨ç¤ºä¾‹æ•°æ®é›†\n2. åŸºäºè®ºæ–‡æ•°æ®ç”Ÿæˆæ–°æ•°æ®é›†\n3. åŠ è½½å·²æœ‰æ•°æ®é›†\nè¯·è¾“å…¥é€‰æ‹© (1-3): ").strip()
    
    if choice == '1':
        # ä½¿ç”¨ç¤ºä¾‹æ•°æ®é›†
        print("ğŸ“„ ä½¿ç”¨ç¤ºä¾‹æ•°æ®é›†...")
        evaluation_dataset = dataset_manager.create_sample_dataset()
        
    elif choice == '2':
        # åŸºäºè®ºæ–‡æ•°æ®ç”Ÿæˆæ•°æ®é›†
        print("ğŸ”„ åŸºäºè®ºæ–‡æ•°æ®ç”Ÿæˆè¯„ä¼°æ•°æ®é›†...")
        papers_data_path = "data/main_system_papers.json"
        
        if not Path(papers_data_path).exists():
            print(f"âŒ è®ºæ–‡æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {papers_data_path}")
            print("è¯·å…ˆè¿è¡ŒPDFå¤„ç†è„šæœ¬ç”Ÿæˆè®ºæ–‡æ•°æ®")
            return
            
        num_questions = int(input("è¾“å…¥è¦ç”Ÿæˆçš„é—®é¢˜æ•°é‡ (æ¨è20-50): ") or "20")
        
        evaluation_dataset = dataset_manager.generate_academic_qa_dataset(
            papers_data_path=papers_data_path,
            num_questions=num_questions,
            llm_base_url="http://localhost:11434",
            model="llama3.1:8b"
        )
        
        # ä¿å­˜ç”Ÿæˆçš„æ•°æ®é›†
        if evaluation_dataset:
            dataset_filename = f"academic_qa_dataset_{len(evaluation_dataset)}.json"
            dataset_manager.save_dataset(evaluation_dataset, dataset_filename)
            print(f"âœ… æ•°æ®é›†å·²ä¿å­˜: {dataset_filename}")
        
    elif choice == '3':
        # åŠ è½½å·²æœ‰æ•°æ®é›†
        print("ğŸ“‚ åŠ è½½å·²æœ‰æ•°æ®é›†...")
        dataset_files = list(Path("data/evaluation").glob("*.json"))
        
        if not dataset_files:
            print("âŒ æœªæ‰¾åˆ°å·²æœ‰æ•°æ®é›†ï¼Œä½¿ç”¨ç¤ºä¾‹æ•°æ®é›†")
            evaluation_dataset = dataset_manager.create_sample_dataset()
        else:
            print("å¯ç”¨æ•°æ®é›†:")
            for i, file in enumerate(dataset_files, 1):
                print(f"   {i}. {file.name}")
            
            file_choice = int(input(f"é€‰æ‹©æ•°æ®é›† (1-{len(dataset_files)}): ") or "1") - 1
            selected_file = dataset_files[file_choice]
            evaluation_dataset = dataset_manager.load_dataset(selected_file.name)
    
    else:
        print("âŒ æ— æ•ˆé€‰æ‹©ï¼Œä½¿ç”¨ç¤ºä¾‹æ•°æ®é›†")
        evaluation_dataset = dataset_manager.create_sample_dataset()
    
    if not evaluation_dataset:
        print("âŒ æ— æ³•åˆ›å»ºè¯„ä¼°æ•°æ®é›†")
        return
    
    print(f"âœ… è¯„ä¼°æ•°æ®é›†å‡†å¤‡å®Œæˆ: {len(evaluation_dataset)} ä¸ªæ¡ˆä¾‹")
    
    # 4. é…ç½®è¯„ä¼°é€‰é¡¹
    print("\nğŸ“‹ è¯„ä¼°é…ç½®:")
    use_llm_eval = input("æ˜¯å¦å¯ç”¨LLMæ·±åº¦è¯„ä¼°? (y/n, é»˜è®¤n): ").lower().startswith('y')
    
    if use_llm_eval:
        print("âš ï¸ æ³¨æ„: LLMè¯„ä¼°ä¼šæ˜¾è‘—å¢åŠ è¯„ä¼°æ—¶é—´")
    
    # 5. å¼€å§‹è¯„ä¼°
    print(f"\nğŸ¯ å¼€å§‹è¯„ä¼°RAGç³»ç»Ÿ...")
    print(f"   æ•°æ®é›†: {len(evaluation_dataset)} ä¸ªæ¡ˆä¾‹")
    print(f"   LLMè¯„ä¼°: {'å¯ç”¨' if use_llm_eval else 'ç¦ç”¨'}")
    print("=" * 80)
    
    try:
        # é€‚é…RAGç³»ç»Ÿçš„queryæ–¹æ³•
        class RAGSystemAdapter:
            def __init__(self, rag_system):
                self.rag_system = rag_system
                
            def query(self, question: str):
                """é€‚é…å™¨æ–¹æ³•ï¼Œç¡®ä¿è¿”å›æ­£ç¡®æ ¼å¼"""
                try:
                    # è°ƒç”¨RAGç³»ç»Ÿ
                    response = self.rag_system.query_academic_papers(question)
                    
                    # æå–ç­”æ¡ˆå’Œä¸Šä¸‹æ–‡
                    if isinstance(response, dict):
                        answer = response.get('answer', '')
                        contexts = []
                        
                        # ä»final_resultsä¸­æå–ä¸Šä¸‹æ–‡
                        final_results = response.get('final_results', [])
                        for result in final_results:
                            if isinstance(result, dict) and 'text' in result:
                                contexts.append(result['text'])
                            elif isinstance(result, str):
                                contexts.append(result)
                        
                        return {
                            'answer': answer,
                            'contexts': contexts
                        }
                    else:
                        return {
                            'answer': str(response),
                            'contexts': []
                        }
                        
                except Exception as e:
                    print(f"   âš ï¸ RAGæŸ¥è¯¢å¤±è´¥: {e}")
                    return {
                        'answer': f"æŸ¥è¯¢å¤±è´¥: {e}",
                        'contexts': []
                    }
        
        # åˆ›å»ºé€‚é…å™¨
        rag_adapter = RAGSystemAdapter(rag_system)
        
        # æ‰§è¡Œè¯„ä¼°
        results = evaluator.evaluate_rag_system(
            rag_system=rag_adapter,
            evaluation_dataset=evaluation_dataset,
            use_llm_evaluation=use_llm_eval,
            save_results=True
        )
        
        print("\nğŸ‰ è¯„ä¼°å®Œæˆ!")
        print("=" * 80)
        
        # æ˜¾ç¤ºç®€è¦ç»“æœ
        summary = results.get('summary', {})
        performance = summary.get('overall_performance', {})
        
        if 'avg_overall_score' in performance:
            print(f"ğŸ“Š RAGç³»ç»Ÿè¯„ä¼°ç»“æœ:")
            print(f"   ğŸ¯ ç»¼åˆè¯„åˆ†: {performance['avg_overall_score']:.3f}/1.0")
            print(f"   ğŸ” ä¸Šä¸‹æ–‡ç›¸å…³æ€§: {performance.get('avg_context_relevance', 0):.3f}/1.0")
            print(f"   ğŸ¤ ç­”æ¡ˆå¿ å®åº¦: {performance.get('avg_faithfulness', 0):.3f}/1.0")
            print(f"   ğŸ’¡ ç­”æ¡ˆç›¸å…³æ€§: {performance.get('avg_answer_relevance', 0):.3f}/1.0")
            print(f"   ğŸ“ˆ ä¸Šä¸‹æ–‡ç²¾ç¡®åº¦: {performance.get('avg_context_precision', 0):.3f}/1.0")
            
            # è¯„ä¼°ç­‰çº§
            overall_score = performance['avg_overall_score']
            if overall_score >= 0.8:
                grade = "ä¼˜ç§€ ğŸ†"
            elif overall_score >= 0.7:
                grade = "è‰¯å¥½ ğŸ‘"
            elif overall_score >= 0.6:
                grade = "ä¸­ç­‰ ğŸ‘Œ"
            elif overall_score >= 0.5:
                grade = "åŠæ ¼ ğŸ“"
            else:
                grade = "éœ€è¦æ”¹è¿› ğŸ“‰"
                
            print(f"   ğŸ“‹ è¯„ä¼°ç­‰çº§: {grade}")
        
        # æ”¹è¿›å»ºè®®
        print(f"\nğŸ’¡ æ”¹è¿›å»ºè®®:")
        if performance.get('avg_context_relevance', 0) < 0.7:
            print("   - ä¼˜åŒ–æ£€ç´¢ç®—æ³•ï¼Œæé«˜ä¸Šä¸‹æ–‡ç›¸å…³æ€§")
        if performance.get('avg_faithfulness', 0) < 0.7:
            print("   - æ”¹è¿›æç¤ºå·¥ç¨‹ï¼Œç¡®ä¿ç­”æ¡ˆåŸºäºä¸Šä¸‹æ–‡")
        if performance.get('avg_answer_relevance', 0) < 0.7:
            print("   - ä¼˜åŒ–é—®ç­”åŒ¹é…ï¼Œæé«˜ç­”æ¡ˆç›¸å…³æ€§")
        if performance.get('avg_context_precision', 0) < 0.6:
            print("   - æé«˜æ£€ç´¢ç²¾ç¡®åº¦ï¼Œè¿‡æ»¤æ— å…³å†…å®¹")
            
        print(f"\nğŸ“ è¯¦ç»†ç»“æœå·²ä¿å­˜åˆ°: data/evaluation/results/")
        
    except KeyboardInterrupt:
        print("\n\nâ¹ï¸ è¯„ä¼°è¢«ç”¨æˆ·ä¸­æ–­")
    except Exception as e:
        print(f"\nâŒ è¯„ä¼°è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
        import traceback
        traceback.print_exc()

def quick_evaluation():
    """å¿«é€Ÿè¯„ä¼°æ¨¡å¼"""
    print("âš¡ å¿«é€Ÿè¯„ä¼°æ¨¡å¼")
    print("=" * 50)
    
    # ä½¿ç”¨ç®€åŒ–é…ç½®
    rag_system = AcademicRAGSystem()
    evaluator = RAGEvaluator(output_dir="data/evaluation/quick_results")
    dataset_manager = BenchmarkDatasets()
    
    # ä½¿ç”¨ç¤ºä¾‹æ•°æ®é›†
    evaluation_dataset = dataset_manager.create_sample_dataset()
    
    class QuickRAGAdapter:
        def __init__(self, rag_system):
            self.rag_system = rag_system
            
        def query(self, question: str):
            try:
                response = self.rag_system.query_academic_papers(question)
                return {
                    'answer': response.get('answer', '') if isinstance(response, dict) else str(response),
                    'contexts': ['Sample context for quick evaluation']
                }
            except:
                return {'answer': 'Quick evaluation answer', 'contexts': []}
    
    rag_adapter = QuickRAGAdapter(rag_system)
    
    results = evaluator.evaluate_rag_system(
        rag_system=rag_adapter,
        evaluation_dataset=evaluation_dataset,
        use_llm_evaluation=False,
        save_results=True
    )
    
    print("âš¡ å¿«é€Ÿè¯„ä¼°å®Œæˆ!")

if __name__ == "__main__":
    if len(sys.argv) > 1 and sys.argv[1] == "--quick":
        quick_evaluation()
    else:
        main()